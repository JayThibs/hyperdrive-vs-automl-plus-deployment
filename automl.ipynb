{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "name": "automl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/hyperdrive-vs-automl-plus-deployment/blob/main/automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjy4Pc9vWXWa"
      },
      "source": [
        "# Automated ML\n",
        "\n",
        "Note: For data exploration, go to hyperparameter_tuning.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCE-mKVP6sCc"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZbdttQy4Un6",
        "gather": {
          "logged": 1617813853477
        },
        "outputId": "87150e8e-2739-4475-d886-73120d9f086c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SDK version: 1.24.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVGom3TYuwx_",
        "gather": {
          "logged": 1617813854338
        }
      },
      "source": [
        "%%writefile feature_preprocessing.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def bools(df):\n",
        "    \"\"\"\n",
        "    public_meeting: we will fill the nulls as 'False'\n",
        "    permit: we will fill the nulls as 'False\n",
        "    \"\"\"\n",
        "    z = ['public_meeting', 'permit']\n",
        "    for i in z:\n",
        "        df[i].fillna(False, inplace = True)\n",
        "        df[i] = df[i].apply(lambda x: float(x))\n",
        "    return df\n",
        "\n",
        "def locs(df, trans = ['longitude', 'latitude', 'gps_height', 'population']):\n",
        "    \"\"\"\n",
        "    fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using medians from \n",
        "    ['subvillage', 'district_code', 'basin'], and lastly the overall median\n",
        "    \"\"\"\n",
        "    df.loc[df.longitude == 0, 'latitude'] = 0\n",
        "    for z in trans:\n",
        "        df[z].replace(0., np.NaN, inplace = True)\n",
        "        df[z].replace(1., np.NaN, inplace = True)\n",
        "        \n",
        "        for j in ['district_code', 'basin']:\n",
        "        \n",
        "            df['median'] = df.groupby([j])[z].transform('median')\n",
        "            df[z] = df[z].fillna(df['median'])\n",
        "        \n",
        "        df[z] = df[z].fillna(df[z].median())\n",
        "        del df['median']\n",
        "    return df\n",
        "\n",
        "def construction(df):\n",
        "    \"\"\"\n",
        "    A lot of null values for construction year. Of course, this is a missing value (a placeholder).\n",
        "    For modeling purposes, this is actually fine, but we'll have trouble with visualizations if we\n",
        "    compare the results for different years, so we'll set the value to something closer to\n",
        "    the other values that aren't placeholders. Let's look at the unique years and set the null\n",
        "    values to 50 years sooner.\n",
        "    Let's set it to 1910 since the lowest \"good\" value is 1960.\n",
        "    \"\"\"\n",
        "    df.loc[df['construction_year'] < 1950, 'construction_year'] = 1910\n",
        "    return df\n",
        "\n",
        "# Alright, now let's drop a few columns\n",
        "\n",
        "def removal(df):\n",
        "  # id: we drop the id column because it is not a useful predictor.\n",
        "  # amount_tsh: is mostly blank - delete\n",
        "  # wpt_name: not useful, delete (too many values)\n",
        "  # subvillage: too many values, delete\n",
        "  # scheme_name: this is almost 50% nulls, so we will delete this column\n",
        "  # num_private: we will delete this column because ~99% of the values are zeros.\n",
        "  features_to_drop = ['id','amount_tsh',  'num_private', \n",
        "          'quantity', 'quality_group', 'source_type', 'payment', \n",
        "          'waterpoint_type_group', 'extraction_type_group', 'wpt_name', \n",
        "          'subvillage', 'scheme_name']\n",
        "  df = df.drop(features_to_drop, axis=1)\n",
        "\n",
        "  return df\n",
        "\n",
        "def dummy(df):\n",
        "    dummy_cols = ['funder', 'installer', 'basin', 'lga', 'ward', 'public_meeting', 'recorded_by',\n",
        "        'scheme_management', 'permit', 'extraction_type',\n",
        "        'extraction_type_class', 'management', 'management_group',\n",
        "        'payment_type', 'water_quality', 'quantity_group', 'source',\n",
        "        'source_class', 'waterpoint_type', 'region']\n",
        "\n",
        "    df = pd.get_dummies(df, columns=dummy_cols)\n",
        "\n",
        "    return df\n",
        "\n",
        "def dates(df):\n",
        "    \"\"\"\n",
        "    date_recorded: this might be a useful variable for this analysis, although the year itself would be useless in a practical scenario moving into the future. We will convert this column into a datetime, and we will also create 'year_recorded' and 'month_recorded' columns just in case those levels prove to be useful. A visual inspection of both casts significant doubt on that possibility, but we'll proceed for now. We will delete date_recorded itself, since random forest cannot accept datetime\n",
        "    \"\"\"\n",
        "    df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n",
        "    df['year_recorded'] = df['date_recorded'].apply(lambda x: x.year)\n",
        "    df['month_recorded'] = df['date_recorded'].apply(lambda x: x.month)\n",
        "    df['date_recorded'] = (pd.to_datetime(df['date_recorded'])).apply(lambda x: x.toordinal())\n",
        "    return df\n",
        "\n",
        "def dates2(df):\n",
        "    \"\"\"\n",
        "    Turn year_recorded and month_recorded into dummy variables\n",
        "    \"\"\"\n",
        "    for z in ['month_recorded', 'year_recorded']:\n",
        "        df[z] = df[z].apply(lambda x: str(x))\n",
        "        good_cols = [z+'_'+i for i in df[z].unique()]\n",
        "        df = pd.concat((df, pd.get_dummies(df[z], prefix = z)[good_cols]), axis = 1)\n",
        "        del df[z]\n",
        "    return df\n",
        "\n",
        "def small_n(df):\n",
        "    \"Collapsing small categorical value counts into 'other'\"\n",
        "    cols = [i for i in df.columns if type(df[i].iloc[0]) == str]\n",
        "    df[cols] = df[cols].where(df[cols].apply(lambda x: x.map(x.value_counts())) > 100, \"other\")\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpdvmaVVWXWg"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "### Overview\n",
        "\n",
        "We'll be using the Pump it Up dataset from the DrivenData competition.\n",
        "\n",
        "The description of the problem: \n",
        "\n",
        "> Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
        "\n",
        "In other words, our goal is to predict which water pumps are non-functioning or functioning, but in need of repair.\n",
        "\n",
        "In this project, we will train a model using AutoML to train multiple multiple and choose the best performing model for deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuj67ZSm4LIE",
        "gather": {
          "logged": 1617816596016
        },
        "outputId": "3a2d67df-0635-4c33-d978-eea734c66aec"
      },
      "source": [
        "# We loaded the dataset into Azure and we are grabbing it here.\n",
        "\n",
        "from azureml.core import Workspace, Experiment, Dataset\n",
        "from feature_preprocessing import *\n",
        "\n",
        "# download config file in azure and put it in the current Notebooks folder\n",
        "ws = Workspace.from_config()\n",
        "exp = Experiment(workspace=ws, name=\"Pump-it-Up-Data-Mining-the-Water-Table\")\n",
        "\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
        "\n",
        "run = exp.start_logging()\n",
        "\n",
        "# download config file in azure and put it in the current Notebooks folder\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "dataset = Dataset.get_by_name(ws, name='Pump-it-Up-dataset')\n",
        "X = dataset.to_pandas_dataframe()\n",
        "y = X['status_group']\n",
        "del X['status_group']\n",
        "\n",
        "# Cleaning up the features of our dataset\n",
        "X = bools(X)\n",
        "X = locs(X)\n",
        "X = construction(X)\n",
        "X = removal(X)\n",
        "X = dummy(X)\n",
        "X = dates(X)\n",
        "x = dates2(X)\n",
        "X = small_n(X)\n",
        "\n",
        "# Removing \">\", \"[\" and \"]\" from the headers to make the data compatible with different algorithms (namely, xgboost)\n",
        "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
        "X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
        "\n",
        "# Converting the population values to log\n",
        "X['population'] = np.log(X['population'])\n",
        "\n",
        "# Splitting the dataset into a training and test set\n",
        "# Test set will be used later\n",
        "# The same random seed (42) for the Hyperdrive model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Concatenating the features and labels together to feed to our AutoML model\n",
        "clean_train_df = pd.concat([X_train, y_train], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Workspace name: quick-starts-ws-142186\n",
            "Azure region: southcentralus\n",
            "Subscription id: f5091c60-1c3c-430f-8d81-d802f6bf2414\n",
            "Resource group: aml-quickstarts-142186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "law5FINf4LIE",
        "gather": {
          "logged": 1617333740668
        }
      },
      "source": [
        "from azureml.data.dataset_factory import TabularDatasetFactory\n",
        "\n",
        "# Get the default datastore to be entered as a parameter in tabular dataset creation\n",
        "datastore = workspace.get_default_datastore()\n",
        "\n",
        "# Change pandas dataframe into a tabular dataset to be used in automl\n",
        "training_data = TabularDatasetFactory.register_pandas_dataframe(clean_train_df, datastore, 'automl_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WChG_Wzp8oXg",
        "gather": {
          "logged": 1617333740981
        }
      },
      "source": [
        "training_data.take(5).to_pandas_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUqJbckB5HD4"
      },
      "source": [
        "# Setting up Experiment\n",
        "\n",
        "We'll create a new experiment for our deployment of an AutoML model and create a project folder to hold the training scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlKWTS0g5GBg",
        "gather": {
          "logged": 1617333761108
        }
      },
      "source": [
        "experiment_name = 'automl-pump-it-up-operationalize'\n",
        "project_folder = './automl-pipeline-project'\n",
        "\n",
        "automl_experiment = Experiment(workspace, experiment_name)\n",
        "automl_experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1617333924178
        },
        "id": "X75NT0jXNVw7",
        "outputId": "156b378d-7a1d-42fc-ca21-92a527385c23"
      },
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Creating a compute cluster if there isn't one that is already created.\n",
        "\n",
        "cpu_cluster_name = 'hypr-auto-clustr'\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ComputeTarget(workspace=workspace, name=cpu_cluster_name)\n",
        "    print('Found existing compute target.')\n",
        "except ComputeTargetException:\n",
        "    print('Creating a new computer target...')\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_v2',\n",
        "                                                          max_nodes=4)\n",
        "    cpu_cluster = ComputeTarget.create(workspace, cpu_cluster_name, compute_config)\n",
        "    \n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'workspace' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a1e443387ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcpu_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComputeTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_cluster_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found existing compute target.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mComputeTargetException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng9WKfEN9FVP"
      },
      "source": [
        "# AutoML Configuration\n",
        "\n",
        "We'll create a new experiment for our deployment of an AutoML model and create a project folder to hold the training scripts.\n",
        "\n",
        "Here we create the general AutoML settings object.\n",
        "\n",
        "\n",
        "Calculate recall to test how well we do on True Positives. We can imagine a real scenario where we want to build a model that does not miss the non-functioning water pumps, and we care much less functioning water pumps that are incorrectly predicted as non-functional. Recall is useful to make sure we miss less True Positives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gather": {
          "logged": 1617334052607
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dYjR1WTR4LIF",
        "outputId": "5c19386c-324f-4e92-976a-6821ce0ddd51"
      },
      "source": [
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "automl_settings = {\n",
        "    \"experiment_timeout_minutes\": 20, # to set a limit on the amount of time AutoML will be running\n",
        "    \"max_concurrent_iterations\": 5, # applies to the compute target we are using\n",
        "    \"primary_metric\" : 'norm_macro_recall' # recall for our performance metric\n",
        "}\n",
        "\n",
        "# Setting AutoML config for model training.\n",
        "\n",
        "automl_config = AutoMLConfig(compute_target=cpu_cluster,\n",
        "                             task = \"classification\", # classifying if water pumps are functional\n",
        "                             training_data=training_data, \n",
        "                             label_column_name=\"status_group\", # our target variable for water pump function  \n",
        "                             path = project_folder,\n",
        "                             enable_early_stopping= True, # prevents automl from spending too much time on models that stopped improving, saves time and compute costs\n",
        "                             featurization= 'auto',\n",
        "                             debug_log = \"automl_errors.log\",\n",
        "                             **automl_settings\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cpu_cluster' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e9937adaf665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Setting AutoML config for model training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m automl_config = AutoMLConfig(compute_target=cpu_cluster,\n\u001b[0m\u001b[1;32m     12\u001b[0m                              \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# classifying if water pumps are functional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                              \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cpu_cluster' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6udoRSe9NW7"
      },
      "source": [
        "## Create Pipeline and AutoMLStep\n",
        "\n",
        "Defining the outputs for the AutoMLStep using TrainingOutput."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xCEtpOZOVS",
        "gather": {
          "logged": 1617334059710
        },
        "outputId": "0061f44f-49d8-4332-a720-31c54d508f09"
      },
      "source": [
        "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
        "\n",
        "ds = workspace.get_default_datastore()\n",
        "metrics_output_name = 'metrics_output'\n",
        "best_model_output_name = 'best_model_output'\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=metrics_output_name,\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "model_data = PipelineData(name='model_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=best_model_output_name,\n",
        "                           training_output=TrainingOutput(type='Model'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'workspace' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1dfa60fa1115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_datastore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmetrics_output_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'metrics_output'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_model_output_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_model_output'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-DVjxDp9dFX"
      },
      "source": [
        "## Create the AutoMLStep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yDcosARZa7K",
        "gather": {
          "logged": 1617334060087
        },
        "outputId": "48a8de5d-1619-41f0-86cc-6665b7d013e0"
      },
      "source": [
        "# Creating an AutoMLStep\n",
        "\n",
        "automl_step = AutoMLStep(\n",
        "    name='automl_module',\n",
        "    automl_config=automl_config,\n",
        "    outputs=[metrics_data, model_data],\n",
        "    allow_reuse=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'automl_config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d53b26a31932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m automl_step = AutoMLStep(\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'automl_module'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mautoml_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoml_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mallow_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'automl_config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8uIv-9iZjo3",
        "gather": {
          "logged": 1617334065457
        },
        "outputId": "4f255dd2-bb36-4c84-edaa-9f7d59c6bb7e"
      },
      "source": [
        "# Creating a Pipeline\n",
        "\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    description=\"pipeline_with_automlstep\",\n",
        "    workspace=workspace,    \n",
        "    steps=[automl_step])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'workspace' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6964c70838c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m pipeline = Pipeline(\n\u001b[1;32m      6\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pipeline_with_automlstep\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     steps=[automl_step])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg-NU8vuZp6Z",
        "gather": {
          "logged": 1617334106966
        },
        "outputId": "1be08aff-48cc-4c7d-d6d5-98475b7ddb3f"
      },
      "source": [
        "print('Submitting AutoML experiment...')\n",
        "\n",
        "pipeline_run = automl_experiment.submit(pipeline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submitting AutoML experiment...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'automl_experiment' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c8b87e3963fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Submitting AutoML experiment...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipeline_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoml_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'automl_experiment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrTPvoOJb4Uq"
      },
      "source": [
        "# Run Details\n",
        "\n",
        "Using the RunDeatils widget to show the different experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wYnNdUEZ_7G",
        "gather": {
          "logged": 1617334149274
        },
        "colab": {
          "referenced_widgets": [
            "288bde079a694ec896843c326d2bfca8"
          ]
        },
        "outputId": "b6155b2b-2abf-4c64-9a75-9b41cc3b1c2f"
      },
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(pipeline_run).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline_run' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c18ecff4a1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline_run' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbk5-oAiaCvS"
      },
      "source": [
        "pipeline_run.wait_for_completion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6onN6m69zUy"
      },
      "source": [
        "# Examine Results\n",
        "\n",
        "# Retrive the metrics of all child runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnfQf6xGabw7"
      },
      "source": [
        "metrics_output = pipeline_run.get_pipeline_output(metrics_output_name)\n",
        "num_file_downloaded = metrics_output.download('.', show_progress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3D5X1aEaeT8"
      },
      "source": [
        "import json\n",
        "with open(metrics_output._path_on_datastore) as f:\n",
        "    metrics_output_result = f.read()\n",
        "    \n",
        "deserialized_metrics_output = json.loads(metrics_output_result)\n",
        "df = pd.DataFrame(deserialized_metrics_output)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C37uiNk-IHp"
      },
      "source": [
        "# Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU9OgmfFae7m",
        "outputId": "4a9436dd-0efc-4652-9be9-573391444f41"
      },
      "source": [
        "# Retrieve best model from Pipeline Run\n",
        "best_model_output = pipeline_run.get_pipeline_output(best_model_output_name)\n",
        "num_file_downloaded = best_model_output.download('.', show_progress=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline_run' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c368596b3d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Retrieve best model from Pipeline Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipeline_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_output_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_file_downloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline_run' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo4EdSyOae-b",
        "outputId": "9c79fac2-93fa-4e8b-f933-154498c41235"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(best_model_output._path_on_datastore, \"rb\" ) as f:\n",
        "    best_model = pickle.load(f)\n",
        "best_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_model_output' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ed1ad7d4ca0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_on_datastore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model_output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMLJGX_afA2",
        "outputId": "05df2679-044e-419c-900c-fef989b0b9de"
      },
      "source": [
        "best_model.steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-50c9d37eaba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0hjMqcq-Z1Y"
      },
      "source": [
        "# Test the model on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkh8yPVAafGG",
        "outputId": "2f82438d-fdaa-44b0-9aa4-3e4184ee8686"
      },
      "source": [
        "# Predict on the Test Set\n",
        "ypred = best_model.predict(X_test)\n",
        "\n",
        "# calculate recall\n",
        "recall = recall_score(y_test, ypred, average='micro')\n",
        "print('Recall: %.3f' % recall)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1872bd423e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict on the Test Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# calculate recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxROfMr3BC-M"
      },
      "source": [
        "# Model Deployment\n",
        "\n",
        "Registering the model, creating an inference config and deploy the model as a web service.\n",
        "\n",
        "In other words, we are publishing the pipeline to enable a REST endpoint to rerun the pipeline from any HTTP library on any platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-M5J1dqar2i",
        "outputId": "cc7c77a8-d1e9-41e3-f12e-e970a534c6c3"
      },
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "    name=\"Pump it Up Train\", description=\"Training Pump it Up pipeline\", version=\"1.0\")\n",
        "\n",
        "published_pipeline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline_run' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-dbb995baef24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m published_pipeline = pipeline_run.publish_pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     name=\"Pump it Up Train\", description=\"Training Pump it Up pipeline\", version=\"1.0\")\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpublished_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline_run' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO9_TzOVMkCC"
      },
      "source": [
        "Now we authenticate to retrieve the auth_header so that the endpoint can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KONZ0u1sar6F",
        "gather": {
          "logged": 1617817936344
        }
      },
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF7As6zVBew1"
      },
      "source": [
        "# Test the Deployed Model\n",
        "\n",
        "Here we will send a request to the deployed model to test it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heGk6iM4ar9f",
        "outputId": "62549589-3af4-4e54-cb35-781457c7b503"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Geting the REST url from the endpoint property of the published pipeline\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "\n",
        "# Building an HTTP POST request to the endpoint\n",
        "# We also add a JSON payload object with the experiment name\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": \"pipeline-rest-endpoint\"}\n",
        "                        )\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.elapsed)\n",
        "print(response.json())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'published_pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2ef521858371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Geting the REST url from the endpoint property of the published pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrest_endpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpublished_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Building an HTTP POST request to the endpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'published_pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnV3Si0PeSPs"
      },
      "source": [
        "headers = {'Content-Type':'application/json'}\n",
        "data = {\"text\": ['the food was horrible', \n",
        "                 'wow, this movie was truely great, I totally enjoyed it!',\n",
        "                 'why the heck was my package not delivered in time?']}\n",
        "\n",
        "resp = requests.post(aci_service.scoring_uri, json=data, headers=headers)\n",
        "print(\"Prediction Results:\", resp.json())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOx0aAUfRO26"
      },
      "source": [
        "We are making it so that a request will trigger the run. We are also going to access the Id key from the response dict to get the value of the run id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpGC1i6LasBZ",
        "outputId": "4272093c-320f-4a68-8820-fe9f1df8e577"
      },
      "source": [
        "try:\n",
        "    response.raise_for_status()\n",
        "except Exception:    \n",
        "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
        "                    \"Response Code: {}\\n\"\n",
        "                    \"Headers: {}\\n\"\n",
        "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n",
        "\n",
        "run_id = response.json().get('Id')\n",
        "print('Submitted pipeline run: ', run_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rest_endpoint' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1df9ca40077b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1df9ca40077b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0;34m\"Response Code: {}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0;34m\"Headers: {}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rest_endpoint' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fddhsrfIRlYJ"
      },
      "source": [
        "# Printing the logs of the web service\n",
        "\n",
        "We can now use the run id to monitor the status of the new run. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhI6b04ldRed",
        "outputId": "c8b86882-f7b6-49db-9399-fda062a24da2"
      },
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments[\"pipeline-rest-endpoint\"], run_id)\n",
        "RunDetails(published_pipeline_run).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ws' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7d892a664673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpublished_pipeline_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipeline-rest-endpoint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublished_pipeline_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ws' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAIMxK1RB9iC"
      },
      "source": [
        "# Printing the logs and Deleting the Service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrhiISos4LIG",
        "outputId": "770032ad-5c3f-4717-9e11-b987c4a0d819"
      },
      "source": [
        "# Delete computer target in order to avoid incurring additional charges.\n",
        "\n",
        "AmlCompute.delete(cpu_cluster)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AmlCompute' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6b6ae0e93abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Delete computer target in order to avoid incurring additional charges.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mAmlCompute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AmlCompute' is not defined"
          ]
        }
      ]
    }
  ]
}