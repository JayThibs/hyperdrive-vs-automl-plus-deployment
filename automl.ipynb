{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "name": "automl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/hyperdrive-vs-automl-plus-deployment/blob/main/automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjy4Pc9vWXWa"
      },
      "source": [
        "# Automated ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCE-mKVP6sCc"
      },
      "source": [
        "# Environment and Import Dependencies\n",
        "\n",
        "Here we specify the conda dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gDTnHm54LH8",
        "outputId": "f9437794-6fb5-4531-b890-32f59817b342"
      },
      "source": [
        "%%writefile conda_dependencies.yml\n",
        "\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- pip=20.2.4\n",
        "- pip:\n",
        "    - azureml-defaults\n",
        "    - scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting conda_dependencies.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npQZM5w74LH9"
      },
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "# Creating a conda environment for model training. It needs to be included in ScriptRunConfig.\n",
        "\n",
        "sklearn_env = Environment.from_conda_specification(name='sklearn_env', file_path='./conda_dependencies.yml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZbdttQy4Un6"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from sklearn import datasets\n",
        "import pkg_resources\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVGom3TYuwx_",
        "outputId": "e9fe9718-0729-471e-a2c9-af748ae1f62c"
      },
      "source": [
        "%%writefile pump_preprocessing.py\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data Exploration was done in a notebook beforehand\n",
        "\n",
        "\n",
        "def removal(X):\n",
        "    \"Removing some columns that aren't to helpful for our model.\"\n",
        "    # id: not a useful predictor\n",
        "    # amount_tsh: mostly blank\n",
        "    # wpt_name: too many values\n",
        "    # subvillage: too many values\n",
        "    # scheme_name: this is almost 50% nulls\n",
        "    # num_private: ~99% of the values are zeros\n",
        "    features_to_drop = ['id', 'num_private', 'wpt_name', 'subvillage', 'scheme_name']\n",
        "    for i in features_to_drop:\n",
        "        del X[i]\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def make_date(df, date_field):\n",
        "    \"Make sure `df[date_field]` is of the right date type.\"\n",
        "    field_dtype = df[date_field].dtype\n",
        "    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
        "        field_dtype = np.datetime64\n",
        "    if not np.issubdtype(field_dtype, np.datetime64):\n",
        "        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n",
        "\n",
        "\n",
        "def add_datepart(df, field_name, prefix=None, drop=True, time=False):\n",
        "    \"Helper function that adds columns relevant to a date in the column `field_name` of `df`.\"\n",
        "    make_date(df, field_name)\n",
        "    field = df[field_name]\n",
        "    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n",
        "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start',\n",
        "            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
        "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
        "    # Pandas removed `dt.week` in v1.1.10\n",
        "    week = field.dt.isocalendar().week.astype(field.dt.day.dtype) if hasattr(field.dt, 'isocalendar') else field.dt.week\n",
        "    for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) if n != 'Week' else week\n",
        "    mask = ~field.isna()\n",
        "    df[prefix + 'Elapsed'] = np.where(mask,field.values.astype(np.int64) // 10 ** 9,np.nan)\n",
        "    if drop: df.drop(field_name, axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def dates(X_train, X_test):\n",
        "    \"\"\"\n",
        "    date_recorded: this might be a useful variable for this analysis, although the year itself would be useless in a practical scenario moving into the future. We will convert this column into a datetime, and we will also create 'year_recorded' and 'month_recorded' columns just in case those levels prove to be useful. A visual inspection of both casts significant doubt on that possibility, but we'll proceed for now. We will delete date_recorded itself, since random forest cannot accept datetime\n",
        "    \"\"\"\n",
        "    for i in [X_train, X_test]:\n",
        "        i['date_recorded'] = pd.to_datetime(i['date_recorded'])\n",
        "        i['year_recorded'] = i['date_recorded'].apply(lambda x: x.year)\n",
        "        i['month_recorded'] = i['date_recorded'].apply(lambda x: x.month)\n",
        "        i['date_recorded'] = (pd.to_datetime(i['date_recorded'])).apply(lambda x: x.toordinal())\n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "def dates2(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Turn year_recorded and month_recorded into dummy variables\n",
        "    \"\"\"\n",
        "    for z in ['month_recorded', 'year_recorded']:\n",
        "        X_train[z] = X_train[z].apply(lambda x: str(x))\n",
        "        X_test[z] = X_test[z].apply(lambda x: str(x))\n",
        "        good_cols = [z+'_'+i for i in X_train[z].unique() if i in X_test[z].unique()]\n",
        "        X_train = pd.concat((X_train, pd.get_dummies(X[z], prefix = z)[good_cols]), axis = 1)\n",
        "        del X[z]\n",
        "    return X\n",
        "\n",
        "\n",
        "def locs(X_train):\n",
        "    \"\"\"\n",
        "    fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using means from \n",
        "    ['subvillage', 'district_code', 'basin'], and lastly the overall mean\n",
        "    \"\"\"\n",
        "    trans = ['longitude', 'latitude', 'gps_height', 'population']\n",
        "    for i in [X]:\n",
        "        i.loc[i.longitude == 0, 'latitude'] = 0\n",
        "    for z in trans:\n",
        "        for i in [X]:\n",
        "            i[z].replace(0., np.NaN, inplace = True)\n",
        "            i[z].replace(1., np.NaN, inplace = True)\n",
        "        \n",
        "        for j in ['subvillage', 'district_code', 'basin']:\n",
        "        \n",
        "            X['mean'] = X.groupby([j])[z].transform('mean')\n",
        "            X[z] = X[z].fillna(X['mean'])\n",
        "        \n",
        "        X[z] = X[z].fillna(X[z].mean())\n",
        "        del X['mean']\n",
        "    return X\n",
        "\n",
        "\n",
        "def bools(X):\n",
        "    \"\"\"\n",
        "    public_meeting: we will fill the nulls as 'False'\n",
        "    permit: we will fill the nulls as 'False'\n",
        "    \"\"\"\n",
        "    z = ['public_meeting', 'permit']\n",
        "    for i in z:\n",
        "        X[i].fillna(False, inplace = True)\n",
        "        X[i] = X[i].apply(lambda x: float(x))\n",
        "    return X\n",
        "\n",
        "\n",
        "def codes(X):\n",
        "    \"\"\"\n",
        "    convert region_code and district_code to string objects, since they are actually categorical variables\n",
        "    \"\"\"\n",
        "    for i in ['region_code', 'district_code']:\n",
        "        X[i] = X[i].apply(lambda x: str(x))\n",
        "    return X\n",
        "\n",
        "\n",
        "def small_n(X):\n",
        "    \"Collapsing small categorical value counts into 'other'\"\n",
        "    cols = [i for i in X.columns if type(X[i].iloc[0]) == str]\n",
        "    X[cols] = X[cols].where(X[cols].apply(lambda x: x.map(x.value_counts())) > 100, \"other\")\n",
        "    return X\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing pump_preprocessing.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpdvmaVVWXWg"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "### Overview\n",
        "\n",
        "We'll be using the Pump it Up dataset from the DrivenData competition.\n",
        "\n",
        "The description of the problem: \n",
        "\n",
        "> Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
        "\n",
        "In other words, our goal is to predict which water pumps are non-functioning or functioning, but in need of repair.\n",
        "\n",
        "In this project, we will train a model using AutoML to train multiple multiple and choose the best performing model for deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuj67ZSm4LIE"
      },
      "source": [
        "from azureml.data.dataset_factory import TabularDatasetFactory\n",
        "\n",
        "# Create TabularDataset using TabularDatasetFactory\n",
        "\n",
        "y = TabularDatasetFactory.from_delimited_files(\"https://drivendata-prod.s3.amazonaws.com/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCYVI2LMPSY%2F20210331%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210331T171733Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ab358ae9d47a9c85a84067b28a636af41d7a7cf71eab3728b869c2b92434884f\")\n",
        "X = TabularDatasetFactory.from_delimited_files(\"https://drivendata-prod.s3.amazonaws.com/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCYVI2LMPSY%2F20210331%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210331T171733Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e414872d8fc9649993f22ca03b4ae7ef7218188bb951de0ef92d0bd271963807\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "gather": {
          "logged": 1598275726969
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "s1EeoYUR4LIE"
      },
      "source": [
        "from pump_preprocessing import *\n",
        "\n",
        "# Removing the id column since it does not help for prediction\n",
        "del y['id']\n",
        "\n",
        "# A lot of null values for construction year. Of course, this is a missing value (a placeholder).\n",
        "# For modeling purposes, this is actually fine, but we'll have trouble with visualizations if we\n",
        "# compare the results for different years, so we'll set the value to something closer to\n",
        "# the other values that aren't placeholders. Let's look at the unique years and set the null\n",
        "# values to 50 years sooner.\n",
        "# Alright, let's set it to 1910\n",
        "X.loc[X['construction_year'] < 1950, 'construction_year'] = 1910\n",
        "X['population'] = np.log(X['population'])\n",
        "\n",
        "# Cleaning up the features of our dataset\n",
        "X = add_datepart(X)\n",
        "X = removal(X)\n",
        "# X = feature_process_helper.dates(X_train)\n",
        "# X = feature_process_helper.dates2(X_train)\n",
        "# X = add_datepart(X_train)\n",
        "X = bools(X_train)\n",
        "X = locs(X_train)\n",
        "X = small_n(X_train)\n",
        "\n",
        "# Splitting the dataset into a training and test set\n",
        "# Test set will be used later\n",
        "# The same random seed (42) for the Hyperdrive model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Concatenating the features and labels together to feed to our AutoML model\n",
        "clean_df = pd.concat([X_train, y_train], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "law5FINf4LIE",
        "outputId": "138b50d8-93cd-43d3-8ccb-0bec92c74507"
      },
      "source": [
        "# Get the default datastore to be entered as a parameter in tabular dataset creation\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "# Change pandas dataframe into a tabular dataset to be used in automl\n",
        "training_data = TabularDatasetFactory.register_pandas_dataframe(clean_df, datastore, 'automl_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method register_pandas_dataframe: This is an experimental method, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validating arguments.\n",
            "Arguments validated.\n",
            "Successfully obtained datastore reference and path.\n",
            "Uploading file to managed-dataset/2fc3f202-3f01-4d90-a565-bec50255e749/\n",
            "Successfully uploaded file to datastore.\n",
            "Creating and registering a new dataset.\n",
            "Successfully created and registered a new dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WChG_Wzp8oXg"
      },
      "source": [
        "training_data.take(5).to_pandas_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUqJbckB5HD4"
      },
      "source": [
        "# Setting up Experiment\n",
        "\n",
        "We'll create a new experiment for our deployment of an AutoML model and create a project folder to hold the training scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlKWTS0g5GBg"
      },
      "source": [
        "experiment_name = 'automl-pump-it-up-operationalize'\n",
        "project_folder = './automl-pipeline-project'\n",
        "\n",
        "automl_experiment = Experiment(ws, experiment_name)\n",
        "automl_experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng9WKfEN9FVP"
      },
      "source": [
        "# AutoML Configuration\n",
        "\n",
        "We'll create a new experiment for our deployment of an AutoML model and create a project folder to hold the training scripts.\n",
        "\n",
        "Here we create the general AutoML settings object.\n",
        "\n",
        "\n",
        "Calculate recall to test how well we do on True Positives. We can imagine a real scenario where we want to build a model that does not miss the non-functioning water pumps, and we care much less functioning water pumps that are incorrectly predicted as non-functional. Recall is useful to make sure we miss less True Positives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "gather": {
          "logged": 1598275665403
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dYjR1WTR4LIF"
      },
      "source": [
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "automl_settings = {\n",
        "    \"experiment_timeout_minutes\": 20, # to set a limit on the amount of time AutoML will be running\n",
        "    \"max_concurrent_iterations\": 5, # applies to the compute target we are using\n",
        "    \"primary_metric\" : 'recall_score_micro' # recall for our performance metric\n",
        "}\n",
        "\n",
        "# Setting AutoML config for model training.\n",
        "\n",
        "automl_config = AutoMLConfig(compute_target=cpu_cluster,\n",
        "                             task = \"classification\", # classifying if water pumps are functional\n",
        "                             training_data=training_data, \n",
        "                             label_column_name=\"status_group\", # our target variable for water pump function  \n",
        "                             path = project_folder,\n",
        "                             enable_early_stopping= True, # prevents automl from spending too much time on models that stopped improving, saves time and compute costs\n",
        "                             featurization= 'auto',\n",
        "                             debug_log = \"automl_errors.log\",\n",
        "                             **automl_settings\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6udoRSe9NW7"
      },
      "source": [
        "## Create Pipeline and AutoMLStep\n",
        "\n",
        "Defining the outputs for the AutoMLStep using TrainingOutput."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xCEtpOZOVS"
      },
      "source": [
        "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
        "\n",
        "ds = ws.get_default_datastore()\n",
        "metrics_output_name = 'metrics_output'\n",
        "best_model_output_name = 'best_model_output'\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=metrics_output_name,\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "model_data = PipelineData(name='model_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=best_model_output_name,\n",
        "                           training_output=TrainingOutput(type='Model'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-DVjxDp9dFX"
      },
      "source": [
        "## Create the AutoMLStep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yDcosARZa7K"
      },
      "source": [
        "# Creating an AutoMLStep\n",
        "\n",
        "automl_step = AutoMLStep(\n",
        "    name='automl_module',\n",
        "    automl_config=automl_config,\n",
        "    outputs=[metrics_data, model_data],\n",
        "    allow_reuse=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8uIv-9iZjo3"
      },
      "source": [
        "# Creating a Pipeline\n",
        "\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    description=\"pipeline_with_automlstep\",\n",
        "    workspace=ws,    \n",
        "    steps=[automl_step])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg-NU8vuZp6Z"
      },
      "source": [
        "print('Submitting AutoML experiment...')\n",
        "\n",
        "pipeline_run = experiment.submit(pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrTPvoOJb4Uq"
      },
      "source": [
        "# Run Details\n",
        "\n",
        "Using the RunDeatils widget to show the different experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wYnNdUEZ_7G"
      },
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(pipeline_run).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbk5-oAiaCvS"
      },
      "source": [
        "pipeline_run.wait_for_completion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6onN6m69zUy"
      },
      "source": [
        "# Examine Results\n",
        "\n",
        "# Retrive the metrics of all child runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnfQf6xGabw7"
      },
      "source": [
        "metrics_output = pipeline_run.get_pipeline_output(metrics_output_name)\n",
        "num_file_downloaded = metrics_output.download('.', show_progress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3D5X1aEaeT8"
      },
      "source": [
        "import json\n",
        "with open(metrics_output._path_on_datastore) as f:\n",
        "    metrics_output_result = f.read()\n",
        "    \n",
        "deserialized_metrics_output = json.loads(metrics_output_result)\n",
        "df = pd.DataFrame(deserialized_metrics_output)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C37uiNk-IHp"
      },
      "source": [
        "# Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU9OgmfFae7m"
      },
      "source": [
        "# Retrieve best model from Pipeline Run\n",
        "best_model_output = pipeline_run.get_pipeline_output(best_model_output_name)\n",
        "num_file_downloaded = best_model_output.download('.', show_progress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo4EdSyOae-b"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(best_model_output._path_on_datastore, \"rb\" ) as f:\n",
        "    best_model = pickle.load(f)\n",
        "best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMLJGX_afA2"
      },
      "source": [
        "best_model.steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0hjMqcq-Z1Y"
      },
      "source": [
        "# Test the model on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkh8yPVAafGG"
      },
      "source": [
        "# Predict on the Test Set\n",
        "ypred = best_model.predict(X_test)\n",
        "\n",
        "# calculate recall\n",
        "recall = recall_score(y_test, ypred, average='micro')\n",
        "print('Recall: %.3f' % recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxROfMr3BC-M"
      },
      "source": [
        "# Model Deployment\n",
        "\n",
        "Registering the model, creating an inference config and deploy the model as a web service.\n",
        "\n",
        "In other words, we are publishing the pipeline to enable a REST endpoint to rerun the pipeline from any HTTP library on any platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-M5J1dqar2i"
      },
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "    name=\"Pump it Up Train\", description=\"Training Pump it Up pipeline\", version=\"1.0\")\n",
        "\n",
        "published_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO9_TzOVMkCC"
      },
      "source": [
        "Now we authenticate to retrieve the auth_header so that the endpoint can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KONZ0u1sar6F"
      },
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF7As6zVBew1"
      },
      "source": [
        "# Test the Deployed Model\n",
        "\n",
        "Here we will send a request to the deployed model to test it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heGk6iM4ar9f"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Geting the REST url from the endpoint property of the published pipeline\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "\n",
        "# Building an HTTP POST request to the endpoint\n",
        "# We also add a JSON payload object with the experiment name\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": \"pipeline-rest-endpoint\"}\n",
        "                        )\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.elapsed)\n",
        "print(response.json())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnV3Si0PeSPs"
      },
      "source": [
        "headers = {'Content-Type':'application/json'}\n",
        "data = {\"text\": ['the food was horrible', \n",
        "                 'wow, this movie was truely great, I totally enjoyed it!',\n",
        "                 'why the heck was my package not delivered in time?']}\n",
        "\n",
        "resp = requests.post(aci_service.scoring_uri, json=data, headers=headers)\n",
        "print(\"Prediction Results:\", resp.json())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOx0aAUfRO26"
      },
      "source": [
        "We are making it so that a request will trigger the run. We are also going to access the Id key from the response dict to get the value of the run id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpGC1i6LasBZ"
      },
      "source": [
        "try:\n",
        "    response.raise_for_status()\n",
        "except Exception:    \n",
        "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
        "                    \"Response Code: {}\\n\"\n",
        "                    \"Headers: {}\\n\"\n",
        "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n",
        "\n",
        "run_id = response.json().get('Id')\n",
        "print('Submitted pipeline run: ', run_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fddhsrfIRlYJ"
      },
      "source": [
        "# Printing the logs of the web service\n",
        "\n",
        "We can now use the run id to monitor the status of the new run. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhI6b04ldRed"
      },
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments[\"pipeline-rest-endpoint\"], run_id)\n",
        "RunDetails(published_pipeline_run).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAIMxK1RB9iC"
      },
      "source": [
        "# Printing the logs and Deleting the Service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrhiISos4LIG"
      },
      "source": [
        "# Delete computer target in order to avoid incurring additional charges.\n",
        "\n",
        "AmlCompute.delete(cpu_cluster)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}