{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/hyperdrive-vs-automl-plus-deployment/blob/main/automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated ML"
      ],
      "metadata": {
        "id": "wjy4Pc9vWXWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment and Import Dependencies\n",
        "\n",
        "Here we specify the conda dependencies."
      ],
      "metadata": {
        "id": "JCE-mKVP6sCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies.yml\n",
        "\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- pip=20.2.4\n",
        "- pip:\n",
        "    - azureml-defaults\n",
        "    - scikit-learn"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting conda_dependencies.yml\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "7gDTnHm54LH8",
        "outputId": "f9437794-6fb5-4531-b890-32f59817b342"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "# Creating a conda environment for model training. It needs to be included in ScriptRunConfig.\n",
        "\n",
        "sklearn_env = Environment.from_conda_specification(name='sklearn_env', file_path='./conda_dependencies.yml')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "npQZM5w74LH9",
        "gather": {
          "logged": 1617813852402
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from sklearn import datasets\n",
        "import pkg_resources\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.24.0\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "id": "nZbdttQy4Un6",
        "gather": {
          "logged": 1617813853477
        },
        "outputId": "87150e8e-2739-4475-d886-73120d9f086c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime\n",
        "\n",
        "# Data Exploration was done in a notebook beforehand\n",
        "\n",
        "\n",
        "def dates(X):\n",
        "    \"\"\"\n",
        "    date_recorded: this might be a useful variable for this analysis, although the year itself would be \n",
        "    useless in a practical scenario moving into the future. We will convert this column into a datetime, \n",
        "    and we will also create 'year_recorded' and 'month_recorded' columns just in case those levels prove \n",
        "    to be useful. A visual inspection of both casts significant doubt on that possibility, but we'll proceed \n",
        "    for now. We will delete date_recorded itself, since random forest cannot accept datetime\n",
        "    \"\"\"\n",
        "    for i in [X]:\n",
        "        i['date_recorded'] = pd.to_datetime(i['date_recorded'])\n",
        "        i['year_recorded'] = i['date_recorded'].apply(lambda x: x.year)\n",
        "        i['month_recorded'] = i['date_recorded'].apply(lambda x: x.month)\n",
        "        i['date_recorded'] = (pd.to_datetime(i['date_recorded'])).apply(lambda x: x.toordinal())\n",
        "    return X\n",
        "\n",
        "\n",
        "def date_parser(df):\n",
        "    date_recorder = list(map(lambda x: datetime.datetime.strptime(str(x), '%Y-%m-%d'),\n",
        "                             df['date_recorded'].values))\n",
        "    df['year_recorder'] = list(map(lambda x: int(x.strftime('%Y')), date_recorder))\n",
        "    df['weekday_recorder'] = list(map(lambda x: int(x.strftime('%w')), date_recorder))\n",
        "    df['yearly_week_recorder'] = list(map(lambda x: int(x.strftime('%W')), date_recorder))\n",
        "    df['month_recorder'] = list(map(lambda x: int(x.strftime('%m')), date_recorder))\n",
        "    df['age'] = df['year_recorder'].values - df['construction_year'].values\n",
        "    del df['date_recorded']\n",
        "    return df\n",
        "\n",
        "\n",
        "def bools(X):\n",
        "    \"\"\"\n",
        "    public_meeting: we will fill the nulls as 'False'\n",
        "    permit: we will fill the nulls as 'False'\n",
        "    \"\"\"\n",
        "    z = ['public_meeting', 'permit']\n",
        "    for i in z:\n",
        "        X[i].fillna(False, inplace = True)\n",
        "        X[i] = X[i].apply(lambda x: float(x))\n",
        "    return X\n",
        "\n",
        "\n",
        "def small_n(X):\n",
        "    \"Collapsing small categorical value counts into 'other'\"\n",
        "    cols = [i for i in X.columns if type(X[i].iloc[0]) == str]\n",
        "    X[cols] = X[cols].where(X[cols].apply(lambda x: x.map(x.value_counts())) > 100, \"other\")\n",
        "    return X\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "tVGom3TYuwx_",
        "gather": {
          "logged": 1617813854338
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "### Overview\n",
        "\n",
        "We'll be using the Pump it Up dataset from the DrivenData competition.\n",
        "\n",
        "The description of the problem: \n",
        "\n",
        "> Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
        "\n",
        "In other words, our goal is to predict which water pumps are non-functioning or functioning, but in need of repair.\n",
        "\n",
        "In this project, we will train a model using AutoML to train multiple multiple and choose the best performing model for deployment."
      ],
      "metadata": {
        "id": "NpdvmaVVWXWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We loaded the dataset into Azure and we are grabbing it here.\n",
        "\n",
        "from azureml.core import Workspace, Experiment\n",
        "\n",
        "# download config file in azure and put it in the current Notebooks folder\n",
        "ws = Workspace.from_config()\n",
        "exp = Experiment(workspace=ws, name=\"Pump-it-Up-Data-Mining-the-Water-Table\")\n",
        "\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
        "\n",
        "run = exp.start_logging()\n",
        "\n",
        "dataset = Dataset.get_by_name(ws, name='Pump-it-Up-dataset')\n",
        "df = dataset.to_pandas_dataframe()\n",
        "y = df['status_group']\n",
        "del df['status_group']\n",
        "X = df\n",
        "del df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workspace name: quick-starts-ws-142186\n",
            "Azure region: southcentralus\n",
            "Subscription id: f5091c60-1c3c-430f-8d81-d802f6bf2414\n",
            "Resource group: aml-quickstarts-142186\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "id": "uuj67ZSm4LIE",
        "gather": {
          "logged": 1617816596016
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "      id  amount_tsh date_recorded        funder  gps_height     installer  \\\n0  69572      6000.0    2011-03-14         Roman        1390         Roman   \n1   8776         0.0    2013-03-06       Grumeti        1399       GRUMETI   \n2  34310        25.0    2013-02-25  Lottery Club         686  World vision   \n3  67743         0.0    2013-01-28        Unicef         263        UNICEF   \n4  19728         0.0    2011-07-13   Action In A           0       Artisan   \n\n   longitude   latitude              wpt_name  num_private  ... payment_type  \\\n0  34.938093  -9.856322                  none            0  ...     annually   \n1  34.698766  -2.147466              Zahanati            0  ...    never pay   \n2  37.460664  -3.821329           Kwa Mahundi            0  ...   per bucket   \n3  38.486161 -11.155298  Zahanati Ya Nanyumbu            0  ...    never pay   \n4  31.130847  -1.825359               Shuleni            0  ...    never pay   \n\n  water_quality quality_group      quantity  quantity_group  \\\n0          soft          good        enough          enough   \n1          soft          good  insufficient    insufficient   \n2          soft          good        enough          enough   \n3          soft          good           dry             dry   \n4          soft          good      seasonal        seasonal   \n\n                 source           source_type  source_class  \\\n0                spring                spring   groundwater   \n1  rainwater harvesting  rainwater harvesting       surface   \n2                   dam                   dam       surface   \n3           machine dbh              borehole   groundwater   \n4  rainwater harvesting  rainwater harvesting       surface   \n\n               waterpoint_type waterpoint_type_group  \n0           communal standpipe    communal standpipe  \n1           communal standpipe    communal standpipe  \n2  communal standpipe multiple    communal standpipe  \n3  communal standpipe multiple    communal standpipe  \n4           communal standpipe    communal standpipe  \n\n[5 rows x 40 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>amount_tsh</th>\n      <th>date_recorded</th>\n      <th>funder</th>\n      <th>gps_height</th>\n      <th>installer</th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>wpt_name</th>\n      <th>num_private</th>\n      <th>...</th>\n      <th>payment_type</th>\n      <th>water_quality</th>\n      <th>quality_group</th>\n      <th>quantity</th>\n      <th>quantity_group</th>\n      <th>source</th>\n      <th>source_type</th>\n      <th>source_class</th>\n      <th>waterpoint_type</th>\n      <th>waterpoint_type_group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69572</td>\n      <td>6000.0</td>\n      <td>2011-03-14</td>\n      <td>Roman</td>\n      <td>1390</td>\n      <td>Roman</td>\n      <td>34.938093</td>\n      <td>-9.856322</td>\n      <td>none</td>\n      <td>0</td>\n      <td>...</td>\n      <td>annually</td>\n      <td>soft</td>\n      <td>good</td>\n      <td>enough</td>\n      <td>enough</td>\n      <td>spring</td>\n      <td>spring</td>\n      <td>groundwater</td>\n      <td>communal standpipe</td>\n      <td>communal standpipe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8776</td>\n      <td>0.0</td>\n      <td>2013-03-06</td>\n      <td>Grumeti</td>\n      <td>1399</td>\n      <td>GRUMETI</td>\n      <td>34.698766</td>\n      <td>-2.147466</td>\n      <td>Zahanati</td>\n      <td>0</td>\n      <td>...</td>\n      <td>never pay</td>\n      <td>soft</td>\n      <td>good</td>\n      <td>insufficient</td>\n      <td>insufficient</td>\n      <td>rainwater harvesting</td>\n      <td>rainwater harvesting</td>\n      <td>surface</td>\n      <td>communal standpipe</td>\n      <td>communal standpipe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34310</td>\n      <td>25.0</td>\n      <td>2013-02-25</td>\n      <td>Lottery Club</td>\n      <td>686</td>\n      <td>World vision</td>\n      <td>37.460664</td>\n      <td>-3.821329</td>\n      <td>Kwa Mahundi</td>\n      <td>0</td>\n      <td>...</td>\n      <td>per bucket</td>\n      <td>soft</td>\n      <td>good</td>\n      <td>enough</td>\n      <td>enough</td>\n      <td>dam</td>\n      <td>dam</td>\n      <td>surface</td>\n      <td>communal standpipe multiple</td>\n      <td>communal standpipe</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>67743</td>\n      <td>0.0</td>\n      <td>2013-01-28</td>\n      <td>Unicef</td>\n      <td>263</td>\n      <td>UNICEF</td>\n      <td>38.486161</td>\n      <td>-11.155298</td>\n      <td>Zahanati Ya Nanyumbu</td>\n      <td>0</td>\n      <td>...</td>\n      <td>never pay</td>\n      <td>soft</td>\n      <td>good</td>\n      <td>dry</td>\n      <td>dry</td>\n      <td>machine dbh</td>\n      <td>borehole</td>\n      <td>groundwater</td>\n      <td>communal standpipe multiple</td>\n      <td>communal standpipe</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19728</td>\n      <td>0.0</td>\n      <td>2011-07-13</td>\n      <td>Action In A</td>\n      <td>0</td>\n      <td>Artisan</td>\n      <td>31.130847</td>\n      <td>-1.825359</td>\n      <td>Shuleni</td>\n      <td>0</td>\n      <td>...</td>\n      <td>never pay</td>\n      <td>soft</td>\n      <td>good</td>\n      <td>seasonal</td>\n      <td>seasonal</td>\n      <td>rainwater harvesting</td>\n      <td>rainwater harvesting</td>\n      <td>surface</td>\n      <td>communal standpipe</td>\n      <td>communal standpipe</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1617816596239
        },
        "id": "TDZijsEtNVw2",
        "outputId": "1dd5fd19-c71d-4241-a6d2-7b84899a3d40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A lot of null values for construction year. Of course, this is a missing value (a placeholder).\n",
        "# For modeling purposes, this is actually fine, but we'll have trouble with visualizations if we\n",
        "# compare the results for different years, so we'll set the value to something closer to\n",
        "# the other values that aren't placeholders. Let's look at the unique years and set the null\n",
        "# values to 50 years sooner.\n",
        "# Alright, let's set it to 1910\n",
        "X.loc[X['construction_year'] < 1950, 'construction_year'] = 1910\n",
        "X['population'] = np.log(X['population'])\n",
        "\n",
        "# Cleaning up the features of our dataset\n",
        "X = dates(X)\n",
        "x = date_parser(X)\n",
        "X = bools(X)\n",
        "X['population'] = np.log(X['population'])\n",
        "X = small_n(X)\n",
        "\n",
        "# Splitting the dataset into a training and test set\n",
        "# Test set will be used later\n",
        "# The same random seed (42) for the Hyperdrive model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Concatenating the features and labels together to feed to our AutoML model\n",
        "clean_df = pd.concat([X_train, y_train], axis=1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/pandas/core/series.py:856: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "time data '734210' does not match format '%Y-%m-%d'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9a63b1c5c167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Cleaning up the features of our dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'population'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'population'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e9a860444e98>\u001b[0m in \u001b[0;36mdate_parser\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     date_recorder = list(map(lambda x: datetime.datetime.strptime(str(x), '%Y-%m-%d'),\n\u001b[0;32m---> 29\u001b[0;31m                              df['date_recorded'].values))\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year_recorder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_recorder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weekday_recorder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_recorder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e9a860444e98>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     date_recorder = list(map(lambda x: datetime.datetime.strptime(str(x), '%Y-%m-%d'),\n\u001b[0m\u001b[1;32m     29\u001b[0m                              df['date_recorded'].values))\n\u001b[1;32m     30\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year_recorder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_recorder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    564\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 362\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
            "\u001b[0;31mValueError\u001b[0m: time data '734210' does not match format '%Y-%m-%d'"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1617333738763
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "s1EeoYUR4LIE",
        "outputId": "3ac68d6e-b9fa-404d-c2a2-4e9b0bc58713"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data.dataset_factory import TabularDatasetFactory\n",
        "\n",
        "# Get the default datastore to be entered as a parameter in tabular dataset creation\n",
        "datastore = workspace.get_default_datastore()\n",
        "\n",
        "# Change pandas dataframe into a tabular dataset to be used in automl\n",
        "training_data = TabularDatasetFactory.register_pandas_dataframe(clean_df, datastore, 'automl_data')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "law5FINf4LIE",
        "gather": {
          "logged": 1617333740668
        },
        "outputId": "138b50d8-93cd-43d3-8ccb-0bec92c74507"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.take(5).to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "WChG_Wzp8oXg",
        "gather": {
          "logged": 1617333740981
        },
        "outputId": "698acbfd-c8c8-4661-9937-7b408c0b4d00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Experiment\n",
        "\n",
        "We'll create a new experiment for our deployment of an AutoML model and create a project folder to hold the training scripts."
      ],
      "metadata": {
        "id": "lUqJbckB5HD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'automl-pump-it-up-operationalize'\n",
        "project_folder = './automl-pipeline-project'\n",
        "\n",
        "automl_experiment = Experiment(workspace, experiment_name)\n",
        "automl_experiment"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZlKWTS0g5GBg",
        "gather": {
          "logged": 1617333761108
        },
        "outputId": "c95d5d4a-9a97-468e-a85c-19739c368192"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Creating a compute cluster if there isn't one that is already created.\n",
        "\n",
        "cpu_cluster_name = 'hypr-auto-clustr'\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ComputeTarget(workspace=workspace, name=cpu_cluster_name)\n",
        "    print('Found existing compute target.')\n",
        "except ComputeTargetException:\n",
        "    print('Creating a new computer target...')\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_v2',\n",
        "                                                          max_nodes=4)\n",
        "    cpu_cluster = ComputeTarget.create(workspace, cpu_cluster_name, compute_config)\n",
        "    \n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'workspace' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a1e443387ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcpu_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComputeTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_cluster_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found existing compute target.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mComputeTargetException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1617333924178
        },
        "id": "X75NT0jXNVw7",
        "outputId": "156b378d-7a1d-42fc-ca21-92a527385c23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoML Configuration\n",
        "\n",
        "We'll create a new experiment for our deployment of an AutoML model and create a project folder to hold the training scripts.\n",
        "\n",
        "Here we create the general AutoML settings object.\n",
        "\n",
        "\n",
        "Calculate recall to test how well we do on True Positives. We can imagine a real scenario where we want to build a model that does not miss the non-functioning water pumps, and we care much less functioning water pumps that are incorrectly predicted as non-functional. Recall is useful to make sure we miss less True Positives."
      ],
      "metadata": {
        "id": "ng9WKfEN9FVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "automl_settings = {\n",
        "    \"experiment_timeout_minutes\": 20, # to set a limit on the amount of time AutoML will be running\n",
        "    \"max_concurrent_iterations\": 5, # applies to the compute target we are using\n",
        "    \"primary_metric\" : 'norm_macro_recall' # recall for our performance metric\n",
        "}\n",
        "\n",
        "# Setting AutoML config for model training.\n",
        "\n",
        "automl_config = AutoMLConfig(compute_target=cpu_cluster,\n",
        "                             task = \"classification\", # classifying if water pumps are functional\n",
        "                             training_data=training_data, \n",
        "                             label_column_name=\"status_group\", # our target variable for water pump function  \n",
        "                             path = project_folder,\n",
        "                             enable_early_stopping= True, # prevents automl from spending too much time on models that stopped improving, saves time and compute costs\n",
        "                             featurization= 'auto',\n",
        "                             debug_log = \"automl_errors.log\",\n",
        "                             **automl_settings\n",
        "                            )"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cpu_cluster' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e9937adaf665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Setting AutoML config for model training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m automl_config = AutoMLConfig(compute_target=cpu_cluster,\n\u001b[0m\u001b[1;32m     12\u001b[0m                              \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# classifying if water pumps are functional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                              \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cpu_cluster' is not defined"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1617334052607
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dYjR1WTR4LIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Pipeline and AutoMLStep\n",
        "\n",
        "Defining the outputs for the AutoMLStep using TrainingOutput."
      ],
      "metadata": {
        "id": "D6udoRSe9NW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
        "\n",
        "ds = workspace.get_default_datastore()\n",
        "metrics_output_name = 'metrics_output'\n",
        "best_model_output_name = 'best_model_output'\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=metrics_output_name,\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "model_data = PipelineData(name='model_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=best_model_output_name,\n",
        "                           training_output=TrainingOutput(type='Model'))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'workspace' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1dfa60fa1115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_datastore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmetrics_output_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'metrics_output'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_model_output_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_model_output'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "id": "9-xCEtpOZOVS",
        "gather": {
          "logged": 1617334059710
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the AutoMLStep"
      ],
      "metadata": {
        "id": "u-DVjxDp9dFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an AutoMLStep\n",
        "\n",
        "automl_step = AutoMLStep(\n",
        "    name='automl_module',\n",
        "    automl_config=automl_config,\n",
        "    outputs=[metrics_data, model_data],\n",
        "    allow_reuse=True\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'automl_config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d53b26a31932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m automl_step = AutoMLStep(\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'automl_module'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mautoml_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoml_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mallow_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'automl_config' is not defined"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "id": "8yDcosARZa7K",
        "gather": {
          "logged": 1617334060087
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Pipeline\n",
        "\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    description=\"pipeline_with_automlstep\",\n",
        "    workspace=workspace,    \n",
        "    steps=[automl_step])"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'workspace' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6964c70838c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m pipeline = Pipeline(\n\u001b[1;32m      6\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pipeline_with_automlstep\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     steps=[automl_step])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'workspace' is not defined"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {
        "id": "D8uIv-9iZjo3",
        "gather": {
          "logged": 1617334065457
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Submitting AutoML experiment...')\n",
        "\n",
        "pipeline_run = automl_experiment.submit(pipeline)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitting AutoML experiment...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'automl_experiment' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c8b87e3963fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Submitting AutoML experiment...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipeline_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoml_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'automl_experiment' is not defined"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "id": "Pg-NU8vuZp6Z",
        "gather": {
          "logged": 1617334106966
        },
        "outputId": "1be08aff-48cc-4c7d-d6d5-98475b7ddb3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Details\n",
        "\n",
        "Using the RunDeatils widget to show the different experiments."
      ],
      "metadata": {
        "id": "xrTPvoOJb4Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(pipeline_run).show()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline_run' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c18ecff4a1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline_run' is not defined"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "4wYnNdUEZ_7G",
        "gather": {
          "logged": 1617334149274
        },
        "outputId": "b6155b2b-2abf-4c64-9a75-9b41cc3b1c2f",
        "colab": {
          "referenced_widgets": [
            "288bde079a694ec896843c326d2bfca8"
          ]
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Mbk5-oAiaCvS",
        "outputId": "77c886a0-92e8-482c-f532-937fecf7b61a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examine Results\n",
        "\n",
        "# Retrive the metrics of all child runs"
      ],
      "metadata": {
        "id": "l6onN6m69zUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_output = pipeline_run.get_pipeline_output(metrics_output_name)\n",
        "num_file_downloaded = metrics_output.download('.', show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vnfQf6xGabw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(metrics_output._path_on_datastore) as f:\n",
        "    metrics_output_result = f.read()\n",
        "    \n",
        "deserialized_metrics_output = json.loads(metrics_output_result)\n",
        "df = pd.DataFrame(deserialized_metrics_output)\n",
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "j3D5X1aEaeT8",
        "outputId": "ed070c4b-8007-4f74-aae7-7bb4f4a69155"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model"
      ],
      "metadata": {
        "id": "4C37uiNk-IHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve best model from Pipeline Run\n",
        "best_model_output = pipeline_run.get_pipeline_output(best_model_output_name)\n",
        "num_file_downloaded = best_model_output.download('.', show_progress=True)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline_run' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-c368596b3d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Retrieve best model from Pipeline Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipeline_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_output_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_file_downloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline_run' is not defined"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "bU9OgmfFae7m",
        "outputId": "4a9436dd-0efc-4652-9be9-573391444f41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(best_model_output._path_on_datastore, \"rb\" ) as f:\n",
        "    best_model = pickle.load(f)\n",
        "best_model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Yo4EdSyOae-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.steps"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-50c9d37eaba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "9RMLJGX_afA2",
        "outputId": "05df2679-044e-419c-900c-fef989b0b9de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on the Test Set"
      ],
      "metadata": {
        "id": "q0hjMqcq-Z1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the Test Set\n",
        "ypred = best_model.predict(X_test)\n",
        "\n",
        "# calculate recall\n",
        "recall = recall_score(y_test, ypred, average='micro')\n",
        "print('Recall: %.3f' % recall)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-1872bd423e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict on the Test Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# calculate recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "Bkh8yPVAafGG",
        "outputId": "2f82438d-fdaa-44b0-9aa4-3e4184ee8686"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Deployment\n",
        "\n",
        "Registering the model, creating an inference config and deploy the model as a web service.\n",
        "\n",
        "In other words, we are publishing the pipeline to enable a REST endpoint to rerun the pipeline from any HTTP library on any platform."
      ],
      "metadata": {
        "id": "TxROfMr3BC-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "    name=\"Pump it Up Train\", description=\"Training Pump it Up pipeline\", version=\"1.0\")\n",
        "\n",
        "published_pipeline"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline_run' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-dbb995baef24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m published_pipeline = pipeline_run.publish_pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     name=\"Pump it Up Train\", description=\"Training Pump it Up pipeline\", version=\"1.0\")\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpublished_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline_run' is not defined"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "_-M5J1dqar2i",
        "outputId": "cc7c77a8-d1e9-41e3-f12e-e970a534c6c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we authenticate to retrieve the auth_header so that the endpoint can be used."
      ],
      "metadata": {
        "id": "sO9_TzOVMkCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KONZ0u1sar6F",
        "gather": {
          "logged": 1617329765988
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Deployed Model\n",
        "\n",
        "Here we will send a request to the deployed model to test it.\n",
        "\n"
      ],
      "metadata": {
        "id": "DF7As6zVBew1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Geting the REST url from the endpoint property of the published pipeline\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "\n",
        "# Building an HTTP POST request to the endpoint\n",
        "# We also add a JSON payload object with the experiment name\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": \"pipeline-rest-endpoint\"}\n",
        "                        )\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.elapsed)\n",
        "print(response.json())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "heGk6iM4ar9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'Content-Type':'application/json'}\n",
        "data = {\"text\": ['the food was horrible', \n",
        "                 'wow, this movie was truely great, I totally enjoyed it!',\n",
        "                 'why the heck was my package not delivered in time?']}\n",
        "\n",
        "resp = requests.post(aci_service.scoring_uri, json=data, headers=headers)\n",
        "print(\"Prediction Results:\", resp.json())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dnV3Si0PeSPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are making it so that a request will trigger the run. We are also going to access the Id key from the response dict to get the value of the run id."
      ],
      "metadata": {
        "id": "VOx0aAUfRO26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    response.raise_for_status()\n",
        "except Exception:    \n",
        "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
        "                    \"Response Code: {}\\n\"\n",
        "                    \"Headers: {}\\n\"\n",
        "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n",
        "\n",
        "run_id = response.json().get('Id')\n",
        "print('Submitted pipeline run: ', run_id)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xpGC1i6LasBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing the logs of the web service\n",
        "\n",
        "We can now use the run id to monitor the status of the new run. "
      ],
      "metadata": {
        "id": "fddhsrfIRlYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments[\"pipeline-rest-endpoint\"], run_id)\n",
        "RunDetails(published_pipeline_run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "OhI6b04ldRed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing the logs and Deleting the Service"
      ],
      "metadata": {
        "id": "dAIMxK1RB9iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete computer target in order to avoid incurring additional charges.\n",
        "\n",
        "AmlCompute.delete(cpu_cluster)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FrhiISos4LIG"
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "name": "automl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}