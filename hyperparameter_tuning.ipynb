{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/hyperdrive-vs-automl-plus-deployment/blob/main/hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using HyperDrive"
      ],
      "metadata": {
        "id": "ab7AZwd-Wa9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment and Import Dependencies\n",
        "\n",
        "Here we specify the conda dependencies."
      ],
      "metadata": {
        "id": "JCE-mKVP6sCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies.yml\n",
        "\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- pip=20.2.4\n",
        "- pip:\n",
        "    - azureml-defaults\n",
        "    - scikit-learn"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting conda_dependencies.yml\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "7gDTnHm54LH8",
        "outputId": "f9437794-6fb5-4531-b890-32f59817b342"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "# Creating a conda environment for model training. It needs to be included in ScriptRunConfig.\n",
        "\n",
        "sklearn_env = Environment.from_conda_specification(name='sklearn_env', file_path='./conda_dependencies.yml')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "npQZM5w74LH9",
        "gather": {
          "logged": 1618085301638
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import csv"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "nZbdttQy4Un6",
        "gather": {
          "logged": 1618085301935
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.24.0\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "id": "uAVsgVw-1QjW",
        "gather": {
          "logged": 1618085302125
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "AJmn0Vl_3rYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile feature_preprocessing.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def bools(df):\n",
        "    \"\"\"\n",
        "    public_meeting: we will fill the nulls as 'False'\n",
        "    permit: we will fill the nulls as 'False\n",
        "    \"\"\"\n",
        "    z = ['public_meeting', 'permit']\n",
        "    for i in z:\n",
        "        df[i].fillna(False, inplace = True)\n",
        "        df[i] = df[i].apply(lambda x: float(x))\n",
        "    return df\n",
        "\n",
        "def locs(df, trans = ['longitude', 'latitude', 'gps_height', 'population']):\n",
        "    \"\"\"\n",
        "    fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using medians from \n",
        "    ['subvillage', 'district_code', 'basin'], and lastly the overall median\n",
        "    \"\"\"\n",
        "    df.loc[df.longitude == 0, 'latitude'] = 0\n",
        "    for z in trans:\n",
        "        df[z].replace(0., np.NaN, inplace = True)\n",
        "        df[z].replace(1., np.NaN, inplace = True)\n",
        "        \n",
        "        for j in ['district_code', 'basin']:\n",
        "        \n",
        "            df['median'] = df.groupby([j])[z].transform('median')\n",
        "            df[z] = df[z].fillna(df['median'])\n",
        "        \n",
        "        df[z] = df[z].fillna(df[z].median())\n",
        "        del df['median']\n",
        "    return df\n",
        "\n",
        "def construction(df):\n",
        "    \"\"\"\n",
        "    A lot of null values for construction year. Of course, this is a missing value (a placeholder).\n",
        "    For modeling purposes, this is actually fine, but we'll have trouble with visualizations if we\n",
        "    compare the results for different years, so we'll set the value to something closer to\n",
        "    the other values that aren't placeholders. Let's look at the unique years and set the null\n",
        "    values to 50 years sooner.\n",
        "    Let's set it to 1910 since the lowest \"good\" value is 1960.\n",
        "    \"\"\"\n",
        "    df.loc[df['construction_year'] < 1950, 'construction_year'] = 1910\n",
        "    return df\n",
        "\n",
        "# Alright, now let's drop a few columns\n",
        "# Needed to drop quite a few categorical columns so that the data would fit in memory in Azure\n",
        "# Tested the model before and after (from 6388 columns to 278) in Colab and only had a ~0.03% reduction in performance\n",
        "\n",
        "def removal(df):\n",
        "  # id: we drop the id column because it is not a useful predictor.\n",
        "  # amount_tsh: is mostly blank - delete\n",
        "  # wpt_name: not useful, delete (too many values)\n",
        "  # subvillage: too many values, delete\n",
        "  # scheme_name: this is almost 50% nulls, so we will delete this column\n",
        "  # num_private: we will delete this column because ~99% of the values are zeros.\n",
        "  features_to_drop = ['id','amount_tsh',  'num_private', \n",
        "          'quantity', 'quality_group', 'source_type', 'payment', \n",
        "          'waterpoint_type_group', 'extraction_type_group', 'wpt_name', \n",
        "          'subvillage', 'scheme_name', 'funder', 'installer', 'recorded_by',\n",
        "          'ward']\n",
        "  df = df.drop(features_to_drop, axis=1)\n",
        "\n",
        "  return df\n",
        "\n",
        "def dummy(df):\n",
        "    dummy_cols = ['basin', 'lga', 'public_meeting',\n",
        "       'scheme_management', 'permit', 'extraction_type',\n",
        "       'extraction_type_class', 'management', 'management_group',\n",
        "       'payment_type', 'water_quality', 'quantity_group', 'source',\n",
        "       'source_class', 'waterpoint_type', 'region']\n",
        "\n",
        "    df = pd.get_dummies(df, columns=dummy_cols)\n",
        "\n",
        "    return df\n",
        "\n",
        "def dates(df):\n",
        "    \"\"\"\n",
        "    date_recorded: this might be a useful variable for this analysis, although the year itself would be useless in a practical scenario moving into the future. We will convert this column into a datetime, and we will also create 'year_recorded' and 'month_recorded' columns just in case those levels prove to be useful. A visual inspection of both casts significant doubt on that possibility, but we'll proceed for now. We will delete date_recorded itself, since random forest cannot accept datetime\n",
        "    \"\"\"\n",
        "    df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n",
        "    df['year_recorded'] = df['date_recorded'].apply(lambda x: x.year)\n",
        "    df['month_recorded'] = df['date_recorded'].apply(lambda x: x.month)\n",
        "    df['date_recorded'] = (pd.to_datetime(df['date_recorded'])).apply(lambda x: x.toordinal())\n",
        "    return df\n",
        "\n",
        "def dates2(df):\n",
        "    \"\"\"\n",
        "    Turn year_recorded and month_recorded into dummy variables\n",
        "    \"\"\"\n",
        "    for z in ['month_recorded', 'year_recorded']:\n",
        "        df[z] = df[z].apply(lambda x: str(x))\n",
        "        good_cols = [z+'_'+i for i in df[z].unique()]\n",
        "        df = pd.concat((df, pd.get_dummies(df[z], prefix = z)[good_cols]), axis = 1)\n",
        "        del df[z]\n",
        "    return df\n",
        "\n",
        "def small_n(df):\n",
        "    \"Collapsing small categorical value counts into 'other'\"\n",
        "    cols = [i for i in df.columns if type(df[i].iloc[0]) == str]\n",
        "    df[cols] = df[cols].where(df[cols].apply(lambda x: x.map(x.value_counts())) > 100, \"other\")\n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting feature_preprocessing.py\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "id": "cVIgOPvy3Ejx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Training File for our Hyperdrive Model"
      ],
      "metadata": {
        "id": "L-WUPuRAXfj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "import os\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "import argparse\n",
        "import joblib\n",
        "from azureml.core.run import Run\n",
        "\n",
        "from feature_preprocessing import *\n",
        "\n",
        "# We loaded the dataset into Azure and we are grabbing it here.\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "# download config file in azure and put it in the current Notebooks folder\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "dataset = Dataset.get_by_name(ws, name='Pump-it-Up-dataset')\n",
        "X = dataset.to_pandas_dataframe()\n",
        "y = X[['status_group']]\n",
        "del X['status_group']\n",
        "\n",
        "# Cleaning up the features of our dataset\n",
        "X = bools(X)\n",
        "X = locs(X)\n",
        "X = construction(X)\n",
        "X = removal(X)\n",
        "X = dummy(X)\n",
        "X = dates(X)\n",
        "x = dates2(X)\n",
        "X = small_n(X)\n",
        "\n",
        "# Removing \">\", \"[\" and \"]\" from the headers to make the data compatible with different algorithms (namely, xgboost)\n",
        "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
        "X.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\n",
        "\n",
        "# Converting the population values to log\n",
        "X['population'] = np.log(X['population'])\n",
        "\n",
        "# Splitting the dataset into a training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "run = Run.get_context()\n",
        "ws = run.experiment.workspace\n",
        "\n",
        "def main():\n",
        "    # Adds arguments to script\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Setting the hyperparameters we will be optimizing for you Random Forest model\n",
        "    parser.add_argument('--max_depth', type=int, default=6, help='The maximum depth of the trees.')\n",
        "    parser.add_argument('--min_samples_split', type=int, default=4, help='The minimum number of samples required to split an internal node.')\n",
        "    parser.add_argument('--n_estimators', type=int, default=750, help='The number of trees in the forest.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run.log(\"Max depth of the trees:\", np.int(args.max_depth))\n",
        "    run.log(\"Minimum number of samples required to split:\", np.int(args.min_samples_split))\n",
        "    run.log(\"Number of trees:\", np.int(args.n_estimators))\n",
        "\n",
        "    # Fitting a Random Forest model to our data. \n",
        "    # Sidenote: I also tried XGBoost on my local machine, but it did not perform as well.\n",
        "    # RF has a score of 0.811, XGBoost has a score of 0.745\n",
        "    # Since I did not use a validation set, it's possible that I'm just overfitting with RF.\n",
        "    # But I wanted to focus on the end-to-end process for this project so I didn't bother with \n",
        "    # a validation set.\n",
        "    rf = RandomForestClassifier(max_depth=args.max_depth,\n",
        "                                min_samples_split=args.min_samples_split,\n",
        "                                n_estimators=args.n_estimators,\n",
        "                                criterion='gini',\n",
        "                                oob_score=True,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1).fit(X_train, y_train.values.ravel())\n",
        "    \n",
        "    # Predicting on the test set\n",
        "    predictions = rf.predict(X_test)\n",
        "    pred = pd.DataFrame(predictions)\n",
        "\n",
        "    # Calculate recall to test how well we do on True Positives\n",
        "    # We can imagine a real scenario where we want to build a model\n",
        "    # that does not miss the non-functioning water pumps, and we\n",
        "    # care much less functioning water pumps that are incorrectly\n",
        "    # predicted as non-functional. \n",
        "    recall_micro = recall_score(y_test, pred, average='micro')\n",
        "    run.log(\"Recall_Micro\", np.float(recall_micro))\n",
        "\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "    joblib.dump(rf, 'outputs/rf_model.pkl')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "id": "qp9Pu3N09Q3Q",
        "outputId": "89fb77c7-ac9e-495c-a2ba-9975fefbe1d9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "Getting our data and initialize a workspace object from persisted configuration. We placed the config file in .\\config.json."
      ],
      "metadata": {
        "id": "3Fmunj8Z5FPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment, Dataset\n",
        "\n",
        "# download config file in azure and put it in the current Notebooks folder\n",
        "ws = Workspace.from_config()\n",
        "exp = Experiment(workspace=ws, name=\"Pump-it-Up-Data-Mining-the-Water-Table\")\n",
        "\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
        "\n",
        "run = exp.start_logging()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workspace name: quick-starts-ws-142332\n",
            "Azure region: southcentralus\n",
            "Subscription id: 1b944a9b-fdae-4f97-aeb1-b7eea0beac53\n",
            "Resource group: aml-quickstarts-142332\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1618085304068
        },
        "id": "LqRY53Mc4LH6",
        "outputId": "1c8697a2-6dff-4b3a-e41f-b261a43bc993"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you get an error during training and try to rerun this cell, it may get stuck in execution.\n",
        "# I'm not sure what's the issue, but it gets resolved by stopping compute before rerunning the notebook.\n",
        "dataset = Dataset.get_by_name(ws, name='Pump-it-Up-dataset')\n",
        "X = dataset.to_pandas_dataframe()\n",
        "y = X[['status_group']]\n",
        "del X['status_group']"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "r00b5L-H-ZdO",
        "gather": {
          "logged": 1618085309016
        },
        "outputId": "270594f0-c91e-4845-fd29-1c5c35f72e6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration\n",
        "\n",
        "Before we dive into training the model with our train.py script, let's do a bit of data exploration to see the steps we took to preprocess the data to prepare it for training."
      ],
      "metadata": {
        "id": "ItX0hlGkt36z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at statistics from the training data\n",
        "# The mean for construction year is definitely off. Looks like there might \n",
        "# be a lot of 0 values if we look at the min and 25% rows.\n",
        "# We can see the same issue for some of the other columns.\n",
        "X.describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "                 id     amount_tsh    gps_height     longitude      latitude  \\\ncount  59400.000000   59396.000000  59400.000000  59400.000000  5.940000e+04   \nmean   37115.131768     317.671762    668.297239     34.077427 -5.706033e+00   \nstd    21453.128371    2997.674361    693.116350      6.567432  2.946019e+00   \nmin        0.000000       0.000000    -90.000000      0.000000 -1.164944e+01   \n25%    18519.750000       0.000000      0.000000     33.090347 -8.540621e+00   \n50%    37061.500000       0.000000    369.000000     34.908743 -5.021597e+00   \n75%    55656.500000      20.000000   1319.250000     37.178387 -3.326156e+00   \nmax    74247.000000  350000.000000   2770.000000     40.345193 -2.000000e-08   \n\n        num_private   region_code  district_code    population  \\\ncount  59400.000000  59400.000000   59400.000000  59400.000000   \nmean       0.474141     15.297003       5.629747    179.909983   \nstd       12.236230     17.587406       9.633649    471.482176   \nmin        0.000000      1.000000       0.000000      0.000000   \n25%        0.000000      5.000000       2.000000      0.000000   \n50%        0.000000     12.000000       3.000000     25.000000   \n75%        0.000000     17.000000       5.000000    215.000000   \nmax     1776.000000     99.000000      80.000000  30500.000000   \n\n       construction_year  \ncount       59400.000000  \nmean         1300.652475  \nstd           951.620547  \nmin             0.000000  \n25%             0.000000  \n50%          1986.000000  \n75%          2004.000000  \nmax          2013.000000  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>amount_tsh</th>\n      <th>gps_height</th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>num_private</th>\n      <th>region_code</th>\n      <th>district_code</th>\n      <th>population</th>\n      <th>construction_year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>59400.000000</td>\n      <td>59396.000000</td>\n      <td>59400.000000</td>\n      <td>59400.000000</td>\n      <td>5.940000e+04</td>\n      <td>59400.000000</td>\n      <td>59400.000000</td>\n      <td>59400.000000</td>\n      <td>59400.000000</td>\n      <td>59400.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>37115.131768</td>\n      <td>317.671762</td>\n      <td>668.297239</td>\n      <td>34.077427</td>\n      <td>-5.706033e+00</td>\n      <td>0.474141</td>\n      <td>15.297003</td>\n      <td>5.629747</td>\n      <td>179.909983</td>\n      <td>1300.652475</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>21453.128371</td>\n      <td>2997.674361</td>\n      <td>693.116350</td>\n      <td>6.567432</td>\n      <td>2.946019e+00</td>\n      <td>12.236230</td>\n      <td>17.587406</td>\n      <td>9.633649</td>\n      <td>471.482176</td>\n      <td>951.620547</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-90.000000</td>\n      <td>0.000000</td>\n      <td>-1.164944e+01</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>18519.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>33.090347</td>\n      <td>-8.540621e+00</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>37061.500000</td>\n      <td>0.000000</td>\n      <td>369.000000</td>\n      <td>34.908743</td>\n      <td>-5.021597e+00</td>\n      <td>0.000000</td>\n      <td>12.000000</td>\n      <td>3.000000</td>\n      <td>25.000000</td>\n      <td>1986.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>55656.500000</td>\n      <td>20.000000</td>\n      <td>1319.250000</td>\n      <td>37.178387</td>\n      <td>-3.326156e+00</td>\n      <td>0.000000</td>\n      <td>17.000000</td>\n      <td>5.000000</td>\n      <td>215.000000</td>\n      <td>2004.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>74247.000000</td>\n      <td>350000.000000</td>\n      <td>2770.000000</td>\n      <td>40.345193</td>\n      <td>-2.000000e-08</td>\n      <td>1776.000000</td>\n      <td>99.000000</td>\n      <td>80.000000</td>\n      <td>30500.000000</td>\n      <td>2013.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "aUXk1ACtrwuZ",
        "gather": {
          "logged": 1618085309283
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of each class\n",
        "y['status_group'].value_counts(normalize = True) * 100"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "functional                 54.308081\nnon functional             38.424242\nfunctional needs repair     7.267677\nName: status_group, dtype: float64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "aKz8sigjrqF6",
        "gather": {
          "logged": 1618085309553
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.columns"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "Index(['id', 'amount_tsh', 'date_recorded', 'funder', 'gps_height',\n       'installer', 'longitude', 'latitude', 'wpt_name', 'num_private',\n       'basin', 'subvillage', 'region', 'region_code', 'district_code', 'lga',\n       'ward', 'population', 'public_meeting', 'recorded_by',\n       'scheme_management', 'scheme_name', 'permit', 'construction_year',\n       'extraction_type', 'extraction_type_group', 'extraction_type_class',\n       'management', 'management_group', 'payment', 'payment_type',\n       'water_quality', 'quality_group', 'quantity', 'quantity_group',\n       'source', 'source_type', 'source_class', 'waterpoint_type',\n       'waterpoint_type_group'],\n      dtype='object')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618085309802
        },
        "id": "BCWAO1HR0Il-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3184933-24d4-4235-8bfe-7195b4b24dfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check to see if any of the categorical columns follow an order (e.g. from low to high)\n",
        "# Spoiler alert: they don't\n",
        "X['quality_group'].unique()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "array(['good', 'salty', 'milky', 'unknown', 'fluoride', 'colored'],\n      dtype=object)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "id": "f861gG5Ar7mF",
        "gather": {
          "logged": 1618085310015
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X['quantity'].unique()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "array(['enough', 'insufficient', 'dry', 'seasonal', 'unknown'],\n      dtype=object)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "id": "6lN4qQGAsIkq",
        "gather": {
          "logged": 1618085310393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique values per column\n",
        "# For categorical values, this is the number of classes\n",
        "print('Total number of examples: ' + str(len(X)))\n",
        "X.nunique().sort_values()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of examples: 59400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "recorded_by                  1\npermit                       2\npublic_meeting               2\nsource_class                 3\nquantity_group               5\nquantity                     5\nmanagement_group             5\nquality_group                6\nwaterpoint_type_group        6\nsource_type                  7\npayment_type                 7\npayment                      7\nwaterpoint_type              7\nextraction_type_class        7\nwater_quality                8\nbasin                        9\nsource                      10\nscheme_management           12\nmanagement                  12\nextraction_type_group       13\nextraction_type             18\ndistrict_code               20\nregion                      21\nregion_code                 27\nconstruction_year           55\nnum_private                 65\namount_tsh                  96\nlga                        125\ndate_recorded              356\npopulation                1049\nfunder                    1897\nward                      2092\ninstaller                 2145\ngps_height                2428\nscheme_name               2696\nsubvillage               19287\nwpt_name                 37400\nlongitude                57516\nlatitude                 57517\nid                       59400\ndtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "id": "had82OwGsNXZ",
        "gather": {
          "logged": 1618085311084
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the number of null values for construction_year\n",
        "len(X[X.construction_year == 0])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "20709"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "id": "M0chNZJXsYdr",
        "gather": {
          "logged": 1618085311440
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alright, let's set it to 1910\n",
        "X.loc[X['construction_year'] < 1950, 'construction_year'] = 1910"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "id": "T8i3uhfgsiLp",
        "gather": {
          "logged": 1618085311627
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the columns we will be dropping in order to improve training\n",
        "features_to_drop = ['id','amount_tsh',  'num_private', \n",
        "          'quantity', 'quality_group', 'source_type', 'payment', \n",
        "          'waterpoint_type_group', 'extraction_type_group', 'wpt_name', \n",
        "          'subvillage', 'scheme_name', 'funder', 'installer', 'recorded_by',\n",
        "          'ward']\n",
        "X = X.drop(features_to_drop, axis=1)\n",
        "X.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "  date_recorded  gps_height  longitude   latitude                    basin  \\\n0    2011-03-14        1390  34.938093  -9.856322               Lake Nyasa   \n1    2013-03-06        1399  34.698766  -2.147466            Lake Victoria   \n2    2013-02-25         686  37.460664  -3.821329                  Pangani   \n3    2013-01-28         263  38.486161 -11.155298  Ruvuma / Southern Coast   \n4    2011-07-13           0  31.130847  -1.825359            Lake Victoria   \n\n    region  region_code  district_code        lga  population  ...  \\\n0   Iringa           11              5     Ludewa         109  ...   \n1     Mara           20              2  Serengeti         280  ...   \n2  Manyara           21              4  Simanjiro         250  ...   \n3   Mtwara           90             63   Nanyumbu          58  ...   \n4   Kagera           18              1    Karagwe           0  ...   \n\n  extraction_type extraction_type_class management  management_group  \\\n0         gravity               gravity        vwc        user-group   \n1         gravity               gravity        wug        user-group   \n2         gravity               gravity        vwc        user-group   \n3     submersible           submersible        vwc        user-group   \n4         gravity               gravity      other             other   \n\n  payment_type water_quality quantity_group                source  \\\n0     annually          soft         enough                spring   \n1    never pay          soft   insufficient  rainwater harvesting   \n2   per bucket          soft         enough                   dam   \n3    never pay          soft            dry           machine dbh   \n4    never pay          soft       seasonal  rainwater harvesting   \n\n  source_class              waterpoint_type  \n0  groundwater           communal standpipe  \n1      surface           communal standpipe  \n2      surface  communal standpipe multiple  \n3  groundwater  communal standpipe multiple  \n4      surface           communal standpipe  \n\n[5 rows x 24 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_recorded</th>\n      <th>gps_height</th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>basin</th>\n      <th>region</th>\n      <th>region_code</th>\n      <th>district_code</th>\n      <th>lga</th>\n      <th>population</th>\n      <th>...</th>\n      <th>extraction_type</th>\n      <th>extraction_type_class</th>\n      <th>management</th>\n      <th>management_group</th>\n      <th>payment_type</th>\n      <th>water_quality</th>\n      <th>quantity_group</th>\n      <th>source</th>\n      <th>source_class</th>\n      <th>waterpoint_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-03-14</td>\n      <td>1390</td>\n      <td>34.938093</td>\n      <td>-9.856322</td>\n      <td>Lake Nyasa</td>\n      <td>Iringa</td>\n      <td>11</td>\n      <td>5</td>\n      <td>Ludewa</td>\n      <td>109</td>\n      <td>...</td>\n      <td>gravity</td>\n      <td>gravity</td>\n      <td>vwc</td>\n      <td>user-group</td>\n      <td>annually</td>\n      <td>soft</td>\n      <td>enough</td>\n      <td>spring</td>\n      <td>groundwater</td>\n      <td>communal standpipe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2013-03-06</td>\n      <td>1399</td>\n      <td>34.698766</td>\n      <td>-2.147466</td>\n      <td>Lake Victoria</td>\n      <td>Mara</td>\n      <td>20</td>\n      <td>2</td>\n      <td>Serengeti</td>\n      <td>280</td>\n      <td>...</td>\n      <td>gravity</td>\n      <td>gravity</td>\n      <td>wug</td>\n      <td>user-group</td>\n      <td>never pay</td>\n      <td>soft</td>\n      <td>insufficient</td>\n      <td>rainwater harvesting</td>\n      <td>surface</td>\n      <td>communal standpipe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2013-02-25</td>\n      <td>686</td>\n      <td>37.460664</td>\n      <td>-3.821329</td>\n      <td>Pangani</td>\n      <td>Manyara</td>\n      <td>21</td>\n      <td>4</td>\n      <td>Simanjiro</td>\n      <td>250</td>\n      <td>...</td>\n      <td>gravity</td>\n      <td>gravity</td>\n      <td>vwc</td>\n      <td>user-group</td>\n      <td>per bucket</td>\n      <td>soft</td>\n      <td>enough</td>\n      <td>dam</td>\n      <td>surface</td>\n      <td>communal standpipe multiple</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2013-01-28</td>\n      <td>263</td>\n      <td>38.486161</td>\n      <td>-11.155298</td>\n      <td>Ruvuma / Southern Coast</td>\n      <td>Mtwara</td>\n      <td>90</td>\n      <td>63</td>\n      <td>Nanyumbu</td>\n      <td>58</td>\n      <td>...</td>\n      <td>submersible</td>\n      <td>submersible</td>\n      <td>vwc</td>\n      <td>user-group</td>\n      <td>never pay</td>\n      <td>soft</td>\n      <td>dry</td>\n      <td>machine dbh</td>\n      <td>groundwater</td>\n      <td>communal standpipe multiple</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011-07-13</td>\n      <td>0</td>\n      <td>31.130847</td>\n      <td>-1.825359</td>\n      <td>Lake Victoria</td>\n      <td>Kagera</td>\n      <td>18</td>\n      <td>1</td>\n      <td>Karagwe</td>\n      <td>0</td>\n      <td>...</td>\n      <td>gravity</td>\n      <td>gravity</td>\n      <td>other</td>\n      <td>other</td>\n      <td>never pay</td>\n      <td>soft</td>\n      <td>seasonal</td>\n      <td>rainwater harvesting</td>\n      <td>surface</td>\n      <td>communal standpipe</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "OqBPIdjY0Il_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "5731a614-7121-4d02-d942-3d53ca47373e",
        "gather": {
          "logged": 1618085311905
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to check which features are objects and then use pd.get_dummies \n",
        "# to turn the features into numerical categories for random forest to work\n",
        "X.columns[X.dtypes==object]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "Index(['basin', 'region', 'lga', 'public_meeting', 'scheme_management',\n       'permit', 'extraction_type', 'extraction_type_class', 'management',\n       'management_group', 'payment_type', 'water_quality', 'quantity_group',\n       'source', 'source_class', 'waterpoint_type'],\n      dtype='object')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618085312201
        },
        "id": "glYuQJhb0Il9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55969ca0-c07d-49fa-f730-52e594200cac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_cols = ['basin', 'lga', 'public_meeting',\n",
        "       'scheme_management', 'permit', 'extraction_type',\n",
        "       'extraction_type_class', 'management', 'management_group',\n",
        "       'payment_type', 'water_quality', 'quantity_group', 'source',\n",
        "       'source_class', 'waterpoint_type', 'region']"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "id": "E7FBv1Cd4tGA",
        "gather": {
          "logged": 1618085312391
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert strings into numerical values\n",
        "# We can see that this dramatically increases the number of features, this is fine\n",
        "# We have 261 features\n",
        "pd.get_dummies(X, columns=dummy_cols)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "      date_recorded  gps_height  longitude   latitude  region_code  \\\n0        2011-03-14        1390  34.938093  -9.856322           11   \n1        2013-03-06        1399  34.698766  -2.147466           20   \n2        2013-02-25         686  37.460664  -3.821329           21   \n3        2013-01-28         263  38.486161 -11.155298           90   \n4        2011-07-13           0  31.130847  -1.825359           18   \n...             ...         ...        ...        ...          ...   \n59395    2013-05-03        1210  37.169807  -3.253847            3   \n59396    2011-05-07        1212  35.249991  -9.070629           11   \n59397    2011-04-11           0  34.017087  -8.750434           12   \n59398    2011-03-08           0  35.861315  -6.378573            1   \n59399    2011-03-23         191  38.104048  -6.747464            5   \n\n       district_code  population  construction_year  basin_Internal  \\\n0                  5         109               1999               0   \n1                  2         280               2010               0   \n2                  4         250               2009               0   \n3                 63          58               1986               0   \n4                  1           0               1910               0   \n...              ...         ...                ...             ...   \n59395              5         125               1999               0   \n59396              4          56               1996               0   \n59397              7           0               1910               0   \n59398              4           0               1910               0   \n59399              2         150               2002               0   \n\n       basin_Lake Nyasa  ...  region_Morogoro  region_Mtwara  region_Mwanza  \\\n0                     1  ...                0              0              0   \n1                     0  ...                0              0              0   \n2                     0  ...                0              0              0   \n3                     0  ...                0              1              0   \n4                     0  ...                0              0              0   \n...                 ...  ...              ...            ...            ...   \n59395                 0  ...                0              0              0   \n59396                 0  ...                0              0              0   \n59397                 0  ...                0              0              0   \n59398                 0  ...                0              0              0   \n59399                 0  ...                1              0              0   \n\n       region_Pwani  region_Rukwa  region_Ruvuma  region_Shinyanga  \\\n0                 0             0              0                 0   \n1                 0             0              0                 0   \n2                 0             0              0                 0   \n3                 0             0              0                 0   \n4                 0             0              0                 0   \n...             ...           ...            ...               ...   \n59395             0             0              0                 0   \n59396             0             0              0                 0   \n59397             0             0              0                 0   \n59398             0             0              0                 0   \n59399             0             0              0                 0   \n\n       region_Singida  region_Tabora  region_Tanga  \n0                   0              0             0  \n1                   0              0             0  \n2                   0              0             0  \n3                   0              0             0  \n4                   0              0             0  \n...               ...            ...           ...  \n59395               0              0             0  \n59396               0              0             0  \n59397               0              0             0  \n59398               0              0             0  \n59399               0              0             0  \n\n[59400 rows x 261 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_recorded</th>\n      <th>gps_height</th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>region_code</th>\n      <th>district_code</th>\n      <th>population</th>\n      <th>construction_year</th>\n      <th>basin_Internal</th>\n      <th>basin_Lake Nyasa</th>\n      <th>...</th>\n      <th>region_Morogoro</th>\n      <th>region_Mtwara</th>\n      <th>region_Mwanza</th>\n      <th>region_Pwani</th>\n      <th>region_Rukwa</th>\n      <th>region_Ruvuma</th>\n      <th>region_Shinyanga</th>\n      <th>region_Singida</th>\n      <th>region_Tabora</th>\n      <th>region_Tanga</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-03-14</td>\n      <td>1390</td>\n      <td>34.938093</td>\n      <td>-9.856322</td>\n      <td>11</td>\n      <td>5</td>\n      <td>109</td>\n      <td>1999</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2013-03-06</td>\n      <td>1399</td>\n      <td>34.698766</td>\n      <td>-2.147466</td>\n      <td>20</td>\n      <td>2</td>\n      <td>280</td>\n      <td>2010</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2013-02-25</td>\n      <td>686</td>\n      <td>37.460664</td>\n      <td>-3.821329</td>\n      <td>21</td>\n      <td>4</td>\n      <td>250</td>\n      <td>2009</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2013-01-28</td>\n      <td>263</td>\n      <td>38.486161</td>\n      <td>-11.155298</td>\n      <td>90</td>\n      <td>63</td>\n      <td>58</td>\n      <td>1986</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011-07-13</td>\n      <td>0</td>\n      <td>31.130847</td>\n      <td>-1.825359</td>\n      <td>18</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1910</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59395</th>\n      <td>2013-05-03</td>\n      <td>1210</td>\n      <td>37.169807</td>\n      <td>-3.253847</td>\n      <td>3</td>\n      <td>5</td>\n      <td>125</td>\n      <td>1999</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59396</th>\n      <td>2011-05-07</td>\n      <td>1212</td>\n      <td>35.249991</td>\n      <td>-9.070629</td>\n      <td>11</td>\n      <td>4</td>\n      <td>56</td>\n      <td>1996</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59397</th>\n      <td>2011-04-11</td>\n      <td>0</td>\n      <td>34.017087</td>\n      <td>-8.750434</td>\n      <td>12</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1910</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59398</th>\n      <td>2011-03-08</td>\n      <td>0</td>\n      <td>35.861315</td>\n      <td>-6.378573</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1910</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59399</th>\n      <td>2011-03-23</td>\n      <td>191</td>\n      <td>38.104048</td>\n      <td>-6.747464</td>\n      <td>5</td>\n      <td>2</td>\n      <td>150</td>\n      <td>2002</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>59400 rows × 261 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "id": "JV_TAvQc3kZl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "9ada19e3-9a98-4128-8178-a0f49f72e8db",
        "gather": {
          "logged": 1618085312653
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On to Training the Model!\n",
        "\n",
        "# But first: Setting up our Compute Target"
      ],
      "metadata": {
        "id": "1aci-_n67fg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Creating a compute cluster if there isn't one that is already created.\n",
        "\n",
        "cpu_cluster_name = 'hypr-auto-clustr'\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
        "    print('Found existing compute target.')\n",
        "except ComputeTargetException:\n",
        "    print('Creating a new computer target...')\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_v2',\n",
        "                                                          max_nodes=4)\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
        "    \n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing compute target.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1618085313685
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "kS4ek0mP4LH-",
        "outputId": "04fee564-830c-4bab-a4fa-92f6cfd65d27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using get_status() to get a detailed status for the current compute cluster.\n",
        "print(cpu_cluster.get_status().serialize())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-04-10T19:46:15.388000+00:00', 'errors': None, 'creationTime': '2021-04-10T18:50:03.293071+00:00', 'modifiedTime': '2021-04-10T18:50:18.729057+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "id": "29YRrdjv4LH_",
        "gather": {
          "logged": 1618085314041
        },
        "outputId": "591db018-9782-4a0d-92b5-1d36df3cca35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_targets = ws.compute_targets\n",
        "for name, ct in compute_targets.items():\n",
        "    print(name, ct.type, ct.provisioning_state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notebook142332 ComputeInstance Succeeded\n",
            "hypr-auto-clustr AmlCompute Succeeded\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "id": "blX8K_h_4LIA",
        "gather": {
          "logged": 1618085314235
        },
        "outputId": "2ef3efcb-0426-4024-c990-24fd383bbd54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_cluster"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "AmlCompute(workspace=Workspace.create(name='quick-starts-ws-142332', subscription_id='1b944a9b-fdae-4f97-aeb1-b7eea0beac53', resource_group='aml-quickstarts-142332'), name=hypr-auto-clustr, id=/subscriptions/1b944a9b-fdae-4f97-aeb1-b7eea0beac53/resourceGroups/aml-quickstarts-142332/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-142332/computes/hypr-auto-clustr, type=AmlCompute, provisioning_state=Succeeded, location=southcentralus, tags=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "id": "7K7tV1ag4LIB",
        "gather": {
          "logged": 1618085314640
        },
        "outputId": "6a342129-d443-4bfb-8f76-716501740f9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperdrive Configuration\n",
        "\n",
        "We are using Random Forest to train our model. We will use Hyperdrive to do a hyperparameter search to optimize our model.\n",
        "\n",
        "We will be using Random Sampling since it is more efficient than grid search for finding an optima.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gk-kpMb97tHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive.policy import BanditPolicy\n",
        "from azureml.train.hyperdrive.sampling import BayesianParameterSampling\n",
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
        "from azureml.train.hyperdrive.parameter_expressions import quniform\n",
        "from azureml.train.hyperdrive.parameter_expressions import choice\n",
        "import os\n",
        "\n",
        "# Specifying parameter sampler.\n",
        "# - Here we use Bayesian Hyperparameter Sampling to search the hyperparameter space for the best model.\n",
        "# - Essentially, Bayesian Sampling builds a probability model of the objective function we are trying \n",
        "#   to minimize and uses it to select the most promising hyperparameters to evaluate in the true objective function.\n",
        "# - For best results with Bayesian Sampling we recommend using a maximum number of runs greater than or \n",
        "#   equal to 20 times the number of hyperparameters being tuned. Recommendend value:60.\n",
        "#   We will be optimizing 3 hyperparameters for this project, therefore we choose 60 max_total_runs.\n",
        "# - We also use quniform to search the hyperparameter space since quniform(low, high, q) creates uniform distriution \n",
        "#   between low and high values, separated by spacing q.\n",
        "\n",
        "ps = BayesianParameterSampling(\n",
        "    {\n",
        "    'max_depth': quniform(3, 12, 3), # Maximum depth of the trees, (3, 6, 9, 12)\n",
        "    'min_samples_split': choice(4, 6), # Minimum number of samples required to split\n",
        "    'n_estimators' : choice(500, 750, 1000) # Number of trees\n",
        "    }\n",
        ")\n",
        "\n",
        "if \"training\" not in os.listdir():\n",
        "    os.mkdir(\"./training\")\n",
        "\n",
        "# Creating a SKLearn estimator for use with train.py\n",
        "src = ScriptRunConfig(source_directory=os.path.join('./'),\n",
        "                      script='train.py',\n",
        "                      compute_target=cpu_cluster,\n",
        "                      environment=sklearn_env)\n",
        "\n",
        "# Creating a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
        "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
        "                                    hyperparameter_sampling=ps,\n",
        "                                    primary_metric_name='Recall_Micro',\n",
        "                                    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "                                    max_total_runs=60,\n",
        "                                    max_concurrent_runs=4)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1618085314859
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "bkWve4Zp4LIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit Experiment and Run Details"
      ],
      "metadata": {
        "id": "2G52h417ACCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submitting a HyperDrive run to the experiment and show run details with the widget.\n",
        "\n",
        "hyperdrive_run = exp.submit(config=hyperdrive_config)\n",
        "\n",
        "RunDetails(hyperdrive_run).show()\n",
        "hyperdrive_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9230a6b5c90845aaa841211a846eba58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"loading\": true}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunId: HD_fc1564e9-0331-466f-b444-39f406fd0772\n",
            "Web View: https://ml.azure.com/experiments/Pump-it-Up-Data-Mining-the-Water-Table/runs/HD_fc1564e9-0331-466f-b444-39f406fd0772?wsid=/subscriptions/1b944a9b-fdae-4f97-aeb1-b7eea0beac53/resourcegroups/aml-quickstarts-142332/workspaces/quick-starts-ws-142332\n",
            "\n",
            "Streaming azureml-logs/hyperdrive.txt\n",
            "=====================================\n",
            "\n",
            "\"<START>[2021-04-10T20:08:33.195788][API][INFO]Experiment created<END>\\n\"\"<START>[2021-04-10T20:08:35.429157][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space<END>\\n\"\"<START>[2021-04-10T20:08:35.739793][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.<END>\\n\"\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "scrolled": false,
        "colab": {
          "referenced_widgets": [
            "759f448677db434fa8ef18faa40b577d",
            "9d306f2f47724b8ca87dac84b16c8d60",
            "36890856a2a54e1b8c916d9a1e563c8d"
          ]
        },
        "id": "u6z3VizC4LIC",
        "gather": {
          "logged": 1618083696172
        },
        "outputId": "073ae2be-0769-4172-846e-9cce80a13302"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving our Best Random Forest / Hyperdrive Model"
      ],
      "metadata": {
        "id": "jiYZr4ucAJfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Get your best run and save the model from that run.\n",
        "\n",
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "best_run_metrics = best_run.get_metrics()\n",
        "\n",
        "print('Best Run:', best_run)\n",
        "print('Metrics:', best_run_metrics['recall_score_micro'])\n",
        "\n",
        "hyperdrive_model = best_run.register_model(model_name=\"rf_hyperdrive_model\", model_path=\"./outputs/model.pkl\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618084686773
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "scrolled": false,
        "id": "DxrY9iQX4LID"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "name": "hyperparameter_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}